{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757b7c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2503cbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sns.load_dataset(\"titanic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d113b7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training DataFrame: (713, 15)\n",
      "Shape of test DataFrame: (178, 15)\n"
     ]
    }
   ],
   "source": [
    "## Randomly select a fraction of the dataset\n",
    "FRACTION = 0.8\n",
    "\n",
    "train_df = df.sample(frac=FRACTION, random_state=42) # fix seed for reproducibility\n",
    "# Get the remaining 20% of rows for the test set\n",
    "# This is achieved by selecting rows whose index is not present in the training set\n",
    "test_df = df.drop(train_df.index)\n",
    "# Display the shapes of the resulting DataFrames\n",
    "print(f\"Shape of training DataFrame: {train_df.shape}\")\n",
    "print(f\"Shape of test DataFrame: {test_df.shape}\")\n",
    "\n",
    "# You can now save these DataFrames if needed\n",
    "train_df.to_csv('train_data.csv', index=False)\n",
    "test_df.to_csv('test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d275525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
       "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
       "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
       "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
       "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
       "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
       "\n",
       "     who  adult_male deck  embark_town alive  alone  \n",
       "0    man        True  NaN  Southampton    no  False  \n",
       "1  woman       False    C    Cherbourg   yes  False  \n",
       "2  woman       False  NaN  Southampton   yes   True  \n",
       "3  woman       False    C  Southampton   yes  False  \n",
       "4    man        True  NaN  Southampton    no   True  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71a263ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "survived         0\n",
       "pclass           0\n",
       "sex              0\n",
       "age            177\n",
       "sibsp            0\n",
       "parch            0\n",
       "fare             0\n",
       "embarked         2\n",
       "class            0\n",
       "who              0\n",
       "adult_male       0\n",
       "deck           688\n",
       "embark_town      2\n",
       "alive            0\n",
       "alone            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "303bbe1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "survived        0.000000\n",
       "pclass          0.000000\n",
       "sex             0.000000\n",
       "age            19.865320\n",
       "sibsp           0.000000\n",
       "parch           0.000000\n",
       "fare            0.000000\n",
       "embarked        0.224467\n",
       "class           0.000000\n",
       "who             0.000000\n",
       "adult_male      0.000000\n",
       "deck           77.216611\n",
       "embark_town     0.224467\n",
       "alive           0.000000\n",
       "alone           0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.isnull().sum() / len(df)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c182c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "survived          int64\n",
      "pclass            int64\n",
      "sex              object\n",
      "age             float64\n",
      "sibsp             int64\n",
      "parch             int64\n",
      "fare            float64\n",
      "embarked         object\n",
      "class          category\n",
      "who              object\n",
      "adult_male         bool\n",
      "deck           category\n",
      "embark_town      object\n",
      "alive            object\n",
      "alone              bool\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Get the data types of all columns\n",
    "column_dtypes = df.dtypes\n",
    "print(column_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f170bb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.heatmap(df.isnull(), cbar=False, yticklabels=False)\n",
    "# plt.title(\"Missing Data Heatmap\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9531561",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop columns with missing value and alive column\n",
    "df_nomissing = df.drop(columns=['age', 'deck', 'embarked', 'embark_town', 'survived',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad7df4d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pclass     sex  sibsp  parch     fare  class    who  adult_male alive  \\\n",
       "0       3    male      1      0   7.2500  Third    man        True    no   \n",
       "1       1  female      1      0  71.2833  First  woman       False   yes   \n",
       "2       3  female      0      0   7.9250  Third  woman       False   yes   \n",
       "3       1  female      1      0  53.1000  First  woman       False   yes   \n",
       "4       3    male      0      0   8.0500  Third    man        True    no   \n",
       "\n",
       "   alone  \n",
       "0  False  \n",
       "1  False  \n",
       "2   True  \n",
       "3  False  \n",
       "4   True  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nomissing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ff6c7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize numeric feature\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "num_cols = [\"fare\", \"sibsp\", \"parch\"]\n",
    "scaler = StandardScaler()\n",
    "df_nomissing[num_cols] = scaler.fit_transform(df_nomissing[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c27a337e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert categorial features to integers using Label Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cat_cols = [\"pclass\", \"sex\", \"class\", \"who\", \"adult_male\", \"alive\", \"alone\"]  # treat pclass as categorical\n",
    "label_encoders = {}\n",
    "\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_nomissing[col] = le.fit_transform(df_nomissing[col])\n",
    "    label_encoders[col] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70c0d032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.502445</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>0.786845</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.488854</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>0.420730</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.486337</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.386671</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.044381</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>2.008933</td>\n",
       "      <td>-0.176263</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.044381</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.492378</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     pclass  sex     sibsp     parch      fare  class  who  adult_male  alive  \\\n",
       "0         2    1  0.432793 -0.473674 -0.502445      2    1           1      0   \n",
       "1         0    0  0.432793 -0.473674  0.786845      0    2           0      1   \n",
       "2         2    0 -0.474545 -0.473674 -0.488854      2    2           0      1   \n",
       "3         0    0  0.432793 -0.473674  0.420730      0    2           0      1   \n",
       "4         2    1 -0.474545 -0.473674 -0.486337      2    1           1      0   \n",
       "..      ...  ...       ...       ...       ...    ...  ...         ...    ...   \n",
       "886       1    1 -0.474545 -0.473674 -0.386671      1    1           1      0   \n",
       "887       0    0 -0.474545 -0.473674 -0.044381      0    2           0      1   \n",
       "888       2    0  0.432793  2.008933 -0.176263      2    2           0      0   \n",
       "889       0    1 -0.474545 -0.473674 -0.044381      0    1           1      1   \n",
       "890       2    1 -0.474545 -0.473674 -0.492378      2    1           1      0   \n",
       "\n",
       "     alone  \n",
       "0        0  \n",
       "1        0  \n",
       "2        1  \n",
       "3        0  \n",
       "4        1  \n",
       "..     ...  \n",
       "886      1  \n",
       "887      1  \n",
       "888      0  \n",
       "889      1  \n",
       "890      1  \n",
       "\n",
       "[891 rows x 10 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nomissing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8137199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training DataFrame: (713, 10)\n",
      "Shape of test DataFrame: (178, 10)\n"
     ]
    }
   ],
   "source": [
    "## Randomly select a fraction of the dataset\n",
    "FRACTION = 0.8\n",
    "\n",
    "train_df_nomissing = df_nomissing.sample(frac=FRACTION, random_state=42) # fix seed for reproducibility\n",
    "# Get the remaining 20% of rows for the test set\n",
    "# This is achieved by selecting rows whose index is not present in the training set\n",
    "test_df_nomissing = df_nomissing.drop(train_df_nomissing.index)\n",
    "# Display the shapes of the resulting DataFrames\n",
    "print(f\"Shape of training DataFrame: {train_df_nomissing.shape}\")\n",
    "print(f\"Shape of test DataFrame: {test_df_nomissing.shape}\")\n",
    "\n",
    "# You can now save these DataFrames if needed\n",
    "train_df_nomissing.to_csv('train_data_nomissing_std.csv', index=False)\n",
    "test_df_nomissing.to_csv('test_data_nomissing_std.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ee10a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df_nomissing.values.astype(\"float32\")\n",
    "X_test = test_df_nomissing.values.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e8b958",
   "metadata": {},
   "source": [
    "# PyTorch Dataset for loading data to training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5d2ee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TitanicAutoencoderDataset(Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.X[idx]  # input = target\n",
    "\n",
    "train_ds = TitanicAutoencoderDataset(X_train)\n",
    "test_ds = TitanicAutoencoderDataset(X_test)\n",
    "\n",
    "# train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "# test_loader = DataLoader(test_ds, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40207522",
   "metadata": {},
   "source": [
    "# Autoencoder model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a6d802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TitanicAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=4):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)       # compressed embedding\n",
    "        x_recon = self.decoder(z) # reconstructed input\n",
    "        return x_recon\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)    # get embedding only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3481d367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curve(train_losses, test_losses):\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.plot(train_losses, label=\"Train Loss\", marker='o')\n",
    "    ax.plot(test_losses, label=\"Test Loss\", marker='x')\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(\"Train vs Test Loss\")\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e09a6628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_gpu_utilization():\n",
    "    result = subprocess.run(\n",
    "        ['nvidia-smi', '--query-gpu=utilization.gpu,memory.used',\n",
    "         '--format=csv,nounits,noheader'],\n",
    "        stdout=subprocess.PIPE,\n",
    "        text=True\n",
    "    )\n",
    "    gpu_util, mem_used = map(int, result.stdout.strip().split(','))\n",
    "    return gpu_util, mem_used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3481496c",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eeb87c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 | Train Loss: 1.2491 | Test Loss: 1.1767 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 1, saving checkpoint...\n",
      "Epoch  2 | Train Loss: 1.2191 | Test Loss: 1.1488 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 2, saving checkpoint...\n",
      "Epoch  3 | Train Loss: 1.1946 | Test Loss: 1.1236 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 3, saving checkpoint...\n",
      "Epoch  4 | Train Loss: 1.1706 | Test Loss: 1.1004 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 4, saving checkpoint...\n",
      "Epoch  5 | Train Loss: 1.1449 | Test Loss: 1.0789 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 5, saving checkpoint...\n",
      "Epoch  6 | Train Loss: 1.1242 | Test Loss: 1.0589 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 6, saving checkpoint...\n",
      "Epoch  7 | Train Loss: 1.1111 | Test Loss: 1.0397 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 7, saving checkpoint...\n",
      "Epoch  8 | Train Loss: 1.0905 | Test Loss: 1.0211 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 8, saving checkpoint...\n",
      "Epoch  9 | Train Loss: 1.0722 | Test Loss: 1.0028 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 9, saving checkpoint...\n",
      "Epoch 10 | Train Loss: 1.0512 | Test Loss: 0.9846 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 10, saving checkpoint...\n",
      "Epoch 11 | Train Loss: 1.0360 | Test Loss: 0.9665 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 11, saving checkpoint...\n",
      "Epoch 12 | Train Loss: 1.0158 | Test Loss: 0.9474 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 12, saving checkpoint...\n",
      "Epoch 13 | Train Loss: 0.9964 | Test Loss: 0.9274 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 13, saving checkpoint...\n",
      "Epoch 14 | Train Loss: 0.9701 | Test Loss: 0.9063 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 14, saving checkpoint...\n",
      "Epoch 15 | Train Loss: 0.9551 | Test Loss: 0.8840 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 15, saving checkpoint...\n",
      "Epoch 16 | Train Loss: 0.9331 | Test Loss: 0.8605 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 16, saving checkpoint...\n",
      "Epoch 17 | Train Loss: 0.9051 | Test Loss: 0.8359 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 17, saving checkpoint...\n",
      "Epoch 18 | Train Loss: 0.8786 | Test Loss: 0.8101 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 18, saving checkpoint...\n",
      "Epoch 19 | Train Loss: 0.8490 | Test Loss: 0.7829 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 19, saving checkpoint...\n",
      "Epoch 20 | Train Loss: 0.8286 | Test Loss: 0.7549 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 20, saving checkpoint...\n",
      "Epoch 21 | Train Loss: 0.7924 | Test Loss: 0.7257 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 21, saving checkpoint...\n",
      "Epoch 22 | Train Loss: 0.7672 | Test Loss: 0.6952 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 22, saving checkpoint...\n",
      "Epoch 23 | Train Loss: 0.7278 | Test Loss: 0.6636 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 23, saving checkpoint...\n",
      "Epoch 24 | Train Loss: 0.7002 | Test Loss: 0.6305 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 24, saving checkpoint...\n",
      "Epoch 25 | Train Loss: 0.6656 | Test Loss: 0.5942 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 25, saving checkpoint...\n",
      "Epoch 26 | Train Loss: 0.6215 | Test Loss: 0.5570 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 26, saving checkpoint...\n",
      "Epoch 27 | Train Loss: 0.5837 | Test Loss: 0.5197 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 27, saving checkpoint...\n",
      "Epoch 28 | Train Loss: 0.5477 | Test Loss: 0.4828 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 28, saving checkpoint...\n",
      "Epoch 29 | Train Loss: 0.5128 | Test Loss: 0.4475 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 29, saving checkpoint...\n",
      "Epoch 30 | Train Loss: 0.4708 | Test Loss: 0.4147 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 30, saving checkpoint...\n",
      "Epoch 31 | Train Loss: 0.4448 | Test Loss: 0.3861 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 31, saving checkpoint...\n",
      "Epoch 32 | Train Loss: 0.4211 | Test Loss: 0.3626 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 32, saving checkpoint...\n",
      "Epoch 33 | Train Loss: 0.4001 | Test Loss: 0.3445 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 33, saving checkpoint...\n",
      "Epoch 34 | Train Loss: 0.3823 | Test Loss: 0.3312 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 34, saving checkpoint...\n",
      "Epoch 35 | Train Loss: 0.3679 | Test Loss: 0.3215 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 35, saving checkpoint...\n",
      "Epoch 36 | Train Loss: 0.3609 | Test Loss: 0.3142 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 36, saving checkpoint...\n",
      "Epoch 37 | Train Loss: 0.3527 | Test Loss: 0.3081 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 37, saving checkpoint...\n",
      "Epoch 38 | Train Loss: 0.3484 | Test Loss: 0.3026 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 38, saving checkpoint...\n",
      "Epoch 39 | Train Loss: 0.3488 | Test Loss: 0.2975 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 39, saving checkpoint...\n",
      "Epoch 40 | Train Loss: 0.3314 | Test Loss: 0.2926 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 40, saving checkpoint...\n",
      "Epoch 41 | Train Loss: 0.3308 | Test Loss: 0.2880 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 41, saving checkpoint...\n",
      "Epoch 42 | Train Loss: 0.3222 | Test Loss: 0.2837 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 42, saving checkpoint...\n",
      "Epoch 43 | Train Loss: 0.3174 | Test Loss: 0.2798 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 43, saving checkpoint...\n",
      "Epoch 44 | Train Loss: 0.3133 | Test Loss: 0.2760 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 44, saving checkpoint...\n",
      "Epoch 45 | Train Loss: 0.3090 | Test Loss: 0.2724 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 45, saving checkpoint...\n",
      "Epoch 46 | Train Loss: 0.3021 | Test Loss: 0.2689 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 46, saving checkpoint...\n",
      "Epoch 47 | Train Loss: 0.2991 | Test Loss: 0.2654 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 47, saving checkpoint...\n",
      "Epoch 48 | Train Loss: 0.2936 | Test Loss: 0.2619 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 48, saving checkpoint...\n",
      "Epoch 49 | Train Loss: 0.2890 | Test Loss: 0.2584 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 49, saving checkpoint...\n",
      "Epoch 50 | Train Loss: 0.2851 | Test Loss: 0.2548 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 50, saving checkpoint...\n",
      "Epoch 51 | Train Loss: 0.2819 | Test Loss: 0.2511 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 51, saving checkpoint...\n",
      "Epoch 52 | Train Loss: 0.2787 | Test Loss: 0.2473 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 52, saving checkpoint...\n",
      "Epoch 53 | Train Loss: 0.2742 | Test Loss: 0.2435 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 53, saving checkpoint...\n",
      "Epoch 54 | Train Loss: 0.2665 | Test Loss: 0.2396 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 54, saving checkpoint...\n",
      "Epoch 55 | Train Loss: 0.2642 | Test Loss: 0.2357 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 55, saving checkpoint...\n",
      "Epoch 56 | Train Loss: 0.2611 | Test Loss: 0.2320 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 56, saving checkpoint...\n",
      "Epoch 57 | Train Loss: 0.2536 | Test Loss: 0.2282 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 57, saving checkpoint...\n",
      "Epoch 58 | Train Loss: 0.2518 | Test Loss: 0.2246 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 58, saving checkpoint...\n",
      "Epoch 59 | Train Loss: 0.2461 | Test Loss: 0.2211 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 59, saving checkpoint...\n",
      "Epoch 60 | Train Loss: 0.2392 | Test Loss: 0.2178 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 60, saving checkpoint...\n",
      "Epoch 61 | Train Loss: 0.2368 | Test Loss: 0.2147 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 61, saving checkpoint...\n",
      "Epoch 62 | Train Loss: 0.2334 | Test Loss: 0.2118 | LR: 0.001000 | Time: 0.10 sec\n",
      "New best model found at epoch 62, saving checkpoint...\n",
      "Epoch 63 | Train Loss: 0.2287 | Test Loss: 0.2090 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 63, saving checkpoint...\n",
      "Epoch 64 | Train Loss: 0.2266 | Test Loss: 0.2063 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 64, saving checkpoint...\n",
      "Epoch 65 | Train Loss: 0.2208 | Test Loss: 0.2040 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 65, saving checkpoint...\n",
      "Epoch 66 | Train Loss: 0.2188 | Test Loss: 0.2018 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 66, saving checkpoint...\n",
      "Epoch 67 | Train Loss: 0.2157 | Test Loss: 0.1999 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 67, saving checkpoint...\n",
      "Epoch 68 | Train Loss: 0.2148 | Test Loss: 0.1982 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 68, saving checkpoint...\n",
      "Epoch 69 | Train Loss: 0.2105 | Test Loss: 0.1966 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 69, saving checkpoint...\n",
      "Epoch 70 | Train Loss: 0.2079 | Test Loss: 0.1952 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 70, saving checkpoint...\n",
      "Epoch 71 | Train Loss: 0.2067 | Test Loss: 0.1939 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 71, saving checkpoint...\n",
      "Epoch 72 | Train Loss: 0.2055 | Test Loss: 0.1928 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 72, saving checkpoint...\n",
      "Epoch 73 | Train Loss: 0.2062 | Test Loss: 0.1917 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 73, saving checkpoint...\n",
      "Epoch 74 | Train Loss: 0.2057 | Test Loss: 0.1907 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 74, saving checkpoint...\n",
      "Epoch 75 | Train Loss: 0.2020 | Test Loss: 0.1897 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 75, saving checkpoint...\n",
      "Epoch 76 | Train Loss: 0.2014 | Test Loss: 0.1888 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 76, saving checkpoint...\n",
      "Epoch 77 | Train Loss: 0.1987 | Test Loss: 0.1880 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 77, saving checkpoint...\n",
      "Epoch 78 | Train Loss: 0.1996 | Test Loss: 0.1871 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 78, saving checkpoint...\n",
      "Epoch 79 | Train Loss: 0.1972 | Test Loss: 0.1862 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 79, saving checkpoint...\n",
      "Epoch 80 | Train Loss: 0.1963 | Test Loss: 0.1853 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 80, saving checkpoint...\n",
      "Epoch 81 | Train Loss: 0.1943 | Test Loss: 0.1844 | LR: 0.001000 | Time: 0.10 sec\n",
      "New best model found at epoch 81, saving checkpoint...\n",
      "Epoch 82 | Train Loss: 0.1931 | Test Loss: 0.1834 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 82, saving checkpoint...\n",
      "Epoch 83 | Train Loss: 0.1927 | Test Loss: 0.1824 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 83, saving checkpoint...\n",
      "Epoch 84 | Train Loss: 0.1907 | Test Loss: 0.1814 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 84, saving checkpoint...\n",
      "Epoch 85 | Train Loss: 0.1927 | Test Loss: 0.1805 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 85, saving checkpoint...\n",
      "Epoch 86 | Train Loss: 0.1906 | Test Loss: 0.1795 | LR: 0.001000 | Time: 1.22 sec\n",
      "New best model found at epoch 86, saving checkpoint...\n",
      "Epoch 87 | Train Loss: 0.1874 | Test Loss: 0.1785 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 87, saving checkpoint...\n",
      "Epoch 88 | Train Loss: 0.1859 | Test Loss: 0.1775 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 88, saving checkpoint...\n",
      "Epoch 89 | Train Loss: 0.1874 | Test Loss: 0.1765 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 89, saving checkpoint...\n",
      "Epoch 90 | Train Loss: 0.1863 | Test Loss: 0.1755 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 90, saving checkpoint...\n",
      "Epoch 91 | Train Loss: 0.1855 | Test Loss: 0.1744 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 91, saving checkpoint...\n",
      "Epoch 92 | Train Loss: 0.1815 | Test Loss: 0.1734 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 92, saving checkpoint...\n",
      "Epoch 93 | Train Loss: 0.1831 | Test Loss: 0.1724 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 93, saving checkpoint...\n",
      "Epoch 94 | Train Loss: 0.1814 | Test Loss: 0.1713 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 94, saving checkpoint...\n",
      "Epoch 95 | Train Loss: 0.1816 | Test Loss: 0.1702 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 95, saving checkpoint...\n",
      "Epoch 96 | Train Loss: 0.1792 | Test Loss: 0.1692 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 96, saving checkpoint...\n",
      "Epoch 97 | Train Loss: 0.1767 | Test Loss: 0.1680 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 97, saving checkpoint...\n",
      "Epoch 98 | Train Loss: 0.1783 | Test Loss: 0.1669 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 98, saving checkpoint...\n",
      "Epoch 99 | Train Loss: 0.1769 | Test Loss: 0.1658 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 99, saving checkpoint...\n",
      "Epoch 100 | Train Loss: 0.1765 | Test Loss: 0.1647 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 100, saving checkpoint...\n",
      "Epoch 101 | Train Loss: 0.1740 | Test Loss: 0.1637 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 101, saving checkpoint...\n",
      "Epoch 102 | Train Loss: 0.1749 | Test Loss: 0.1625 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 102, saving checkpoint...\n",
      "Epoch 103 | Train Loss: 0.1743 | Test Loss: 0.1614 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 103, saving checkpoint...\n",
      "Epoch 104 | Train Loss: 0.1723 | Test Loss: 0.1603 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 104, saving checkpoint...\n",
      "Epoch 105 | Train Loss: 0.1724 | Test Loss: 0.1591 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 105, saving checkpoint...\n",
      "Epoch 106 | Train Loss: 0.1696 | Test Loss: 0.1580 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 106, saving checkpoint...\n",
      "Epoch 107 | Train Loss: 0.1692 | Test Loss: 0.1567 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 107, saving checkpoint...\n",
      "Epoch 108 | Train Loss: 0.1676 | Test Loss: 0.1557 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 108, saving checkpoint...\n",
      "Epoch 109 | Train Loss: 0.1649 | Test Loss: 0.1545 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 109, saving checkpoint...\n",
      "Epoch 110 | Train Loss: 0.1663 | Test Loss: 0.1533 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 110, saving checkpoint...\n",
      "Epoch 111 | Train Loss: 0.1643 | Test Loss: 0.1520 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 111, saving checkpoint...\n",
      "Epoch 112 | Train Loss: 0.1639 | Test Loss: 0.1509 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 112, saving checkpoint...\n",
      "Epoch 113 | Train Loss: 0.1612 | Test Loss: 0.1496 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 113, saving checkpoint...\n",
      "Epoch 114 | Train Loss: 0.1612 | Test Loss: 0.1486 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 114, saving checkpoint...\n",
      "Epoch 115 | Train Loss: 0.1597 | Test Loss: 0.1475 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 115, saving checkpoint...\n",
      "Epoch 116 | Train Loss: 0.1589 | Test Loss: 0.1463 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 116, saving checkpoint...\n",
      "Epoch 117 | Train Loss: 0.1567 | Test Loss: 0.1452 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 117, saving checkpoint...\n",
      "Epoch 118 | Train Loss: 0.1554 | Test Loss: 0.1439 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 118, saving checkpoint...\n",
      "Epoch 119 | Train Loss: 0.1558 | Test Loss: 0.1428 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 119, saving checkpoint...\n",
      "Epoch 120 | Train Loss: 0.1546 | Test Loss: 0.1417 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 120, saving checkpoint...\n",
      "Epoch 121 | Train Loss: 0.1517 | Test Loss: 0.1405 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 121, saving checkpoint...\n",
      "Epoch 122 | Train Loss: 0.1529 | Test Loss: 0.1394 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 122, saving checkpoint...\n",
      "Epoch 123 | Train Loss: 0.1508 | Test Loss: 0.1383 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 123, saving checkpoint...\n",
      "Epoch 124 | Train Loss: 0.1514 | Test Loss: 0.1373 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 124, saving checkpoint...\n",
      "Epoch 125 | Train Loss: 0.1497 | Test Loss: 0.1362 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 125, saving checkpoint...\n",
      "Epoch 126 | Train Loss: 0.1479 | Test Loss: 0.1351 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 126, saving checkpoint...\n",
      "Epoch 127 | Train Loss: 0.1468 | Test Loss: 0.1341 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 127, saving checkpoint...\n",
      "Epoch 128 | Train Loss: 0.1448 | Test Loss: 0.1331 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 128, saving checkpoint...\n",
      "Epoch 129 | Train Loss: 0.1450 | Test Loss: 0.1321 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 129, saving checkpoint...\n",
      "Epoch 130 | Train Loss: 0.1427 | Test Loss: 0.1311 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 130, saving checkpoint...\n",
      "Epoch 131 | Train Loss: 0.1451 | Test Loss: 0.1302 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 131, saving checkpoint...\n",
      "Epoch 132 | Train Loss: 0.1411 | Test Loss: 0.1293 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 132, saving checkpoint...\n",
      "Epoch 133 | Train Loss: 0.1419 | Test Loss: 0.1283 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 133, saving checkpoint...\n",
      "Epoch 134 | Train Loss: 0.1398 | Test Loss: 0.1273 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 134, saving checkpoint...\n",
      "Epoch 135 | Train Loss: 0.1375 | Test Loss: 0.1264 | LR: 0.001000 | Time: 0.31 sec\n",
      "New best model found at epoch 135, saving checkpoint...\n",
      "Epoch 136 | Train Loss: 0.1365 | Test Loss: 0.1255 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 136, saving checkpoint...\n",
      "Epoch 137 | Train Loss: 0.1375 | Test Loss: 0.1246 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 137, saving checkpoint...\n",
      "Epoch 138 | Train Loss: 0.1367 | Test Loss: 0.1238 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 138, saving checkpoint...\n",
      "Epoch 139 | Train Loss: 0.1355 | Test Loss: 0.1231 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 139, saving checkpoint...\n",
      "Epoch 140 | Train Loss: 0.1336 | Test Loss: 0.1222 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 140, saving checkpoint...\n",
      "Epoch 141 | Train Loss: 0.1331 | Test Loss: 0.1213 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 141, saving checkpoint...\n",
      "Epoch 142 | Train Loss: 0.1334 | Test Loss: 0.1204 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 142, saving checkpoint...\n",
      "Epoch 143 | Train Loss: 0.1318 | Test Loss: 0.1195 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 143, saving checkpoint...\n",
      "Epoch 144 | Train Loss: 0.1307 | Test Loss: 0.1187 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 144, saving checkpoint...\n",
      "Epoch 145 | Train Loss: 0.1304 | Test Loss: 0.1180 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 145, saving checkpoint...\n",
      "Epoch 146 | Train Loss: 0.1294 | Test Loss: 0.1175 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 146, saving checkpoint...\n",
      "Epoch 147 | Train Loss: 0.1291 | Test Loss: 0.1167 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 147, saving checkpoint...\n",
      "Epoch 148 | Train Loss: 0.1279 | Test Loss: 0.1159 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 148, saving checkpoint...\n",
      "Epoch 149 | Train Loss: 0.1264 | Test Loss: 0.1150 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 149, saving checkpoint...\n",
      "Epoch 150 | Train Loss: 0.1265 | Test Loss: 0.1140 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 150, saving checkpoint...\n",
      "Epoch 151 | Train Loss: 0.1255 | Test Loss: 0.1131 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 151, saving checkpoint...\n",
      "Epoch 152 | Train Loss: 0.1237 | Test Loss: 0.1123 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 152, saving checkpoint...\n",
      "Epoch 153 | Train Loss: 0.1230 | Test Loss: 0.1117 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 153, saving checkpoint...\n",
      "Epoch 154 | Train Loss: 0.1226 | Test Loss: 0.1111 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 154, saving checkpoint...\n",
      "Epoch 155 | Train Loss: 0.1217 | Test Loss: 0.1103 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 155, saving checkpoint...\n",
      "Epoch 156 | Train Loss: 0.1201 | Test Loss: 0.1094 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 156, saving checkpoint...\n",
      "Epoch 157 | Train Loss: 0.1187 | Test Loss: 0.1085 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 157, saving checkpoint...\n",
      "Epoch 158 | Train Loss: 0.1189 | Test Loss: 0.1077 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 158, saving checkpoint...\n",
      "Epoch 159 | Train Loss: 0.1175 | Test Loss: 0.1070 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 159, saving checkpoint...\n",
      "Epoch 160 | Train Loss: 0.1161 | Test Loss: 0.1062 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 160, saving checkpoint...\n",
      "Epoch 161 | Train Loss: 0.1156 | Test Loss: 0.1055 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 161, saving checkpoint...\n",
      "Epoch 162 | Train Loss: 0.1167 | Test Loss: 0.1046 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 162, saving checkpoint...\n",
      "Epoch 163 | Train Loss: 0.1131 | Test Loss: 0.1034 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 163, saving checkpoint...\n",
      "Epoch 164 | Train Loss: 0.1137 | Test Loss: 0.1023 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 164, saving checkpoint...\n",
      "Epoch 165 | Train Loss: 0.1126 | Test Loss: 0.1015 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 165, saving checkpoint...\n",
      "Epoch 166 | Train Loss: 0.1112 | Test Loss: 0.1006 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 166, saving checkpoint...\n",
      "Epoch 167 | Train Loss: 0.1096 | Test Loss: 0.0997 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 167, saving checkpoint...\n",
      "Epoch 168 | Train Loss: 0.1096 | Test Loss: 0.0987 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 168, saving checkpoint...\n",
      "Epoch 169 | Train Loss: 0.1076 | Test Loss: 0.0978 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 169, saving checkpoint...\n",
      "Epoch 170 | Train Loss: 0.1085 | Test Loss: 0.0967 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 170, saving checkpoint...\n",
      "Epoch 171 | Train Loss: 0.1059 | Test Loss: 0.0956 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 171, saving checkpoint...\n",
      "Epoch 172 | Train Loss: 0.1042 | Test Loss: 0.0945 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 172, saving checkpoint...\n",
      "Epoch 173 | Train Loss: 0.1040 | Test Loss: 0.0935 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 173, saving checkpoint...\n",
      "Epoch 174 | Train Loss: 0.1025 | Test Loss: 0.0921 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 174, saving checkpoint...\n",
      "Epoch 175 | Train Loss: 0.1005 | Test Loss: 0.0910 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 175, saving checkpoint...\n",
      "Epoch 176 | Train Loss: 0.0996 | Test Loss: 0.0898 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 176, saving checkpoint...\n",
      "Epoch 177 | Train Loss: 0.0975 | Test Loss: 0.0887 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 177, saving checkpoint...\n",
      "Epoch 178 | Train Loss: 0.0961 | Test Loss: 0.0875 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 178, saving checkpoint...\n",
      "Epoch 179 | Train Loss: 0.0949 | Test Loss: 0.0863 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 179, saving checkpoint...\n",
      "Epoch 180 | Train Loss: 0.0939 | Test Loss: 0.0849 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 180, saving checkpoint...\n",
      "Epoch 181 | Train Loss: 0.0935 | Test Loss: 0.0834 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 181, saving checkpoint...\n",
      "Epoch 182 | Train Loss: 0.0905 | Test Loss: 0.0819 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 182, saving checkpoint...\n",
      "Epoch 183 | Train Loss: 0.0892 | Test Loss: 0.0805 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 183, saving checkpoint...\n",
      "Epoch 184 | Train Loss: 0.0883 | Test Loss: 0.0790 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 184, saving checkpoint...\n",
      "Epoch 185 | Train Loss: 0.0863 | Test Loss: 0.0776 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 185, saving checkpoint...\n",
      "Epoch 186 | Train Loss: 0.0840 | Test Loss: 0.0763 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 186, saving checkpoint...\n",
      "Epoch 187 | Train Loss: 0.0834 | Test Loss: 0.0749 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 187, saving checkpoint...\n",
      "Epoch 188 | Train Loss: 0.0807 | Test Loss: 0.0734 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 188, saving checkpoint...\n",
      "Epoch 189 | Train Loss: 0.0800 | Test Loss: 0.0718 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 189, saving checkpoint...\n",
      "Epoch 190 | Train Loss: 0.0788 | Test Loss: 0.0703 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 190, saving checkpoint...\n",
      "Epoch 191 | Train Loss: 0.0772 | Test Loss: 0.0689 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 191, saving checkpoint...\n",
      "Epoch 192 | Train Loss: 0.0754 | Test Loss: 0.0675 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 192, saving checkpoint...\n",
      "Epoch 193 | Train Loss: 0.0740 | Test Loss: 0.0662 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 193, saving checkpoint...\n",
      "Epoch 194 | Train Loss: 0.0713 | Test Loss: 0.0647 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 194, saving checkpoint...\n",
      "Epoch 195 | Train Loss: 0.0714 | Test Loss: 0.0633 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 195, saving checkpoint...\n",
      "Epoch 196 | Train Loss: 0.0690 | Test Loss: 0.0620 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 196, saving checkpoint...\n",
      "Epoch 197 | Train Loss: 0.0683 | Test Loss: 0.0609 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 197, saving checkpoint...\n",
      "Epoch 198 | Train Loss: 0.0671 | Test Loss: 0.0597 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 198, saving checkpoint...\n",
      "Epoch 199 | Train Loss: 0.0661 | Test Loss: 0.0588 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 199, saving checkpoint...\n",
      "Epoch 200 | Train Loss: 0.0646 | Test Loss: 0.0578 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 200, saving checkpoint...\n",
      "Epoch 201 | Train Loss: 0.0638 | Test Loss: 0.0568 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 201, saving checkpoint...\n",
      "Epoch 202 | Train Loss: 0.0625 | Test Loss: 0.0558 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 202, saving checkpoint...\n",
      "Epoch 203 | Train Loss: 0.0621 | Test Loss: 0.0550 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 203, saving checkpoint...\n",
      "Epoch 204 | Train Loss: 0.0609 | Test Loss: 0.0542 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 204, saving checkpoint...\n",
      "Epoch 205 | Train Loss: 0.0601 | Test Loss: 0.0535 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 205, saving checkpoint...\n",
      "Epoch 206 | Train Loss: 0.0599 | Test Loss: 0.0529 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 206, saving checkpoint...\n",
      "Epoch 207 | Train Loss: 0.0586 | Test Loss: 0.0523 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 207, saving checkpoint...\n",
      "Epoch 208 | Train Loss: 0.0582 | Test Loss: 0.0517 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 208, saving checkpoint...\n",
      "Epoch 209 | Train Loss: 0.0579 | Test Loss: 0.0512 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 209, saving checkpoint...\n",
      "Epoch 210 | Train Loss: 0.0571 | Test Loss: 0.0505 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 210, saving checkpoint...\n",
      "Epoch 211 | Train Loss: 0.0560 | Test Loss: 0.0501 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 211, saving checkpoint...\n",
      "Epoch 212 | Train Loss: 0.0559 | Test Loss: 0.0496 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 212, saving checkpoint...\n",
      "Epoch 213 | Train Loss: 0.0555 | Test Loss: 0.0490 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 213, saving checkpoint...\n",
      "Epoch 214 | Train Loss: 0.0549 | Test Loss: 0.0486 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 214, saving checkpoint...\n",
      "Epoch 215 | Train Loss: 0.0540 | Test Loss: 0.0481 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 215, saving checkpoint...\n",
      "Epoch 216 | Train Loss: 0.0543 | Test Loss: 0.0477 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 216, saving checkpoint...\n",
      "Epoch 217 | Train Loss: 0.0537 | Test Loss: 0.0473 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 217, saving checkpoint...\n",
      "Epoch 218 | Train Loss: 0.0534 | Test Loss: 0.0469 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 218, saving checkpoint...\n",
      "Epoch 219 | Train Loss: 0.0528 | Test Loss: 0.0465 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 219, saving checkpoint...\n",
      "Epoch 220 | Train Loss: 0.0528 | Test Loss: 0.0461 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 220, saving checkpoint...\n",
      "Epoch 221 | Train Loss: 0.0519 | Test Loss: 0.0457 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 221, saving checkpoint...\n",
      "Epoch 222 | Train Loss: 0.0514 | Test Loss: 0.0454 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 222, saving checkpoint...\n",
      "Epoch 223 | Train Loss: 0.0513 | Test Loss: 0.0451 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 223, saving checkpoint...\n",
      "Epoch 224 | Train Loss: 0.0510 | Test Loss: 0.0447 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 224, saving checkpoint...\n",
      "Epoch 225 | Train Loss: 0.0507 | Test Loss: 0.0445 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 225, saving checkpoint...\n",
      "Epoch 226 | Train Loss: 0.0504 | Test Loss: 0.0442 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 226, saving checkpoint...\n",
      "Epoch 227 | Train Loss: 0.0503 | Test Loss: 0.0438 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 227, saving checkpoint...\n",
      "Epoch 228 | Train Loss: 0.0499 | Test Loss: 0.0435 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 228, saving checkpoint...\n",
      "Epoch 229 | Train Loss: 0.0494 | Test Loss: 0.0432 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 229, saving checkpoint...\n",
      "Epoch 230 | Train Loss: 0.0494 | Test Loss: 0.0430 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 230, saving checkpoint...\n",
      "Epoch 231 | Train Loss: 0.0491 | Test Loss: 0.0427 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 231, saving checkpoint...\n",
      "Epoch 232 | Train Loss: 0.0486 | Test Loss: 0.0425 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 232, saving checkpoint...\n",
      "Epoch 233 | Train Loss: 0.0484 | Test Loss: 0.0423 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 233, saving checkpoint...\n",
      "Epoch 234 | Train Loss: 0.0484 | Test Loss: 0.0421 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 234, saving checkpoint...\n",
      "Epoch 235 | Train Loss: 0.0479 | Test Loss: 0.0419 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 235, saving checkpoint...\n",
      "Epoch 236 | Train Loss: 0.0479 | Test Loss: 0.0416 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 236, saving checkpoint...\n",
      "Epoch 237 | Train Loss: 0.0479 | Test Loss: 0.0414 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 237, saving checkpoint...\n",
      "Epoch 238 | Train Loss: 0.0475 | Test Loss: 0.0412 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 238, saving checkpoint...\n",
      "Epoch 239 | Train Loss: 0.0469 | Test Loss: 0.0410 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 239, saving checkpoint...\n",
      "Epoch 240 | Train Loss: 0.0470 | Test Loss: 0.0408 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 240, saving checkpoint...\n",
      "Epoch 241 | Train Loss: 0.0471 | Test Loss: 0.0405 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 241, saving checkpoint...\n",
      "Epoch 242 | Train Loss: 0.0464 | Test Loss: 0.0404 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 242, saving checkpoint...\n",
      "Epoch 243 | Train Loss: 0.0462 | Test Loss: 0.0402 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 243, saving checkpoint...\n",
      "Epoch 244 | Train Loss: 0.0459 | Test Loss: 0.0400 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 244, saving checkpoint...\n",
      "Epoch 245 | Train Loss: 0.0460 | Test Loss: 0.0399 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 245, saving checkpoint...\n",
      "Epoch 246 | Train Loss: 0.0458 | Test Loss: 0.0397 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 246, saving checkpoint...\n",
      "Epoch 247 | Train Loss: 0.0458 | Test Loss: 0.0395 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 247, saving checkpoint...\n",
      "Epoch 248 | Train Loss: 0.0458 | Test Loss: 0.0393 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 248, saving checkpoint...\n",
      "Epoch 249 | Train Loss: 0.0455 | Test Loss: 0.0391 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 249, saving checkpoint...\n",
      "Epoch 250 | Train Loss: 0.0451 | Test Loss: 0.0390 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 250, saving checkpoint...\n",
      "Epoch 251 | Train Loss: 0.0448 | Test Loss: 0.0388 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 251, saving checkpoint...\n",
      "Epoch 252 | Train Loss: 0.0449 | Test Loss: 0.0386 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 252, saving checkpoint...\n",
      "Epoch 253 | Train Loss: 0.0443 | Test Loss: 0.0385 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 253, saving checkpoint...\n",
      "Epoch 254 | Train Loss: 0.0442 | Test Loss: 0.0383 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 254, saving checkpoint...\n",
      "Epoch 255 | Train Loss: 0.0446 | Test Loss: 0.0382 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 255, saving checkpoint...\n",
      "Epoch 256 | Train Loss: 0.0444 | Test Loss: 0.0380 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 256, saving checkpoint...\n",
      "Epoch 257 | Train Loss: 0.0438 | Test Loss: 0.0378 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 257, saving checkpoint...\n",
      "Epoch 258 | Train Loss: 0.0437 | Test Loss: 0.0376 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 258, saving checkpoint...\n",
      "Epoch 259 | Train Loss: 0.0438 | Test Loss: 0.0375 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 259, saving checkpoint...\n",
      "Epoch 260 | Train Loss: 0.0434 | Test Loss: 0.0373 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 260, saving checkpoint...\n",
      "Epoch 261 | Train Loss: 0.0431 | Test Loss: 0.0372 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 261, saving checkpoint...\n",
      "Epoch 262 | Train Loss: 0.0429 | Test Loss: 0.0371 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 262, saving checkpoint...\n",
      "Epoch 263 | Train Loss: 0.0430 | Test Loss: 0.0369 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 263, saving checkpoint...\n",
      "Epoch 264 | Train Loss: 0.0429 | Test Loss: 0.0367 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 264, saving checkpoint...\n",
      "Epoch 265 | Train Loss: 0.0424 | Test Loss: 0.0366 | LR: 0.001000 | Time: 0.41 sec\n",
      "New best model found at epoch 265, saving checkpoint...\n",
      "Epoch 266 | Train Loss: 0.0424 | Test Loss: 0.0364 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 266, saving checkpoint...\n",
      "Epoch 267 | Train Loss: 0.0422 | Test Loss: 0.0363 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 267, saving checkpoint...\n",
      "Epoch 268 | Train Loss: 0.0425 | Test Loss: 0.0362 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 268, saving checkpoint...\n",
      "Epoch 269 | Train Loss: 0.0425 | Test Loss: 0.0360 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 269, saving checkpoint...\n",
      "Epoch 270 | Train Loss: 0.0421 | Test Loss: 0.0359 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 270, saving checkpoint...\n",
      "Epoch 271 | Train Loss: 0.0421 | Test Loss: 0.0358 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 271, saving checkpoint...\n",
      "Epoch 272 | Train Loss: 0.0422 | Test Loss: 0.0357 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 272, saving checkpoint...\n",
      "Epoch 273 | Train Loss: 0.0417 | Test Loss: 0.0356 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 273, saving checkpoint...\n",
      "Epoch 274 | Train Loss: 0.0415 | Test Loss: 0.0354 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 274, saving checkpoint...\n",
      "Epoch 275 | Train Loss: 0.0414 | Test Loss: 0.0353 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 275, saving checkpoint...\n",
      "Epoch 276 | Train Loss: 0.0413 | Test Loss: 0.0352 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 276, saving checkpoint...\n",
      "Epoch 277 | Train Loss: 0.0410 | Test Loss: 0.0351 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 277, saving checkpoint...\n",
      "Epoch 278 | Train Loss: 0.0410 | Test Loss: 0.0350 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 278, saving checkpoint...\n",
      "Epoch 279 | Train Loss: 0.0405 | Test Loss: 0.0349 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 279, saving checkpoint...\n",
      "Epoch 280 | Train Loss: 0.0405 | Test Loss: 0.0348 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 280, saving checkpoint...\n",
      "Epoch 281 | Train Loss: 0.0405 | Test Loss: 0.0347 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 281, saving checkpoint...\n",
      "Epoch 282 | Train Loss: 0.0407 | Test Loss: 0.0346 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 282, saving checkpoint...\n",
      "Epoch 283 | Train Loss: 0.0405 | Test Loss: 0.0344 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 283, saving checkpoint...\n",
      "Epoch 284 | Train Loss: 0.0405 | Test Loss: 0.0343 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 284, saving checkpoint...\n",
      "Epoch 285 | Train Loss: 0.0403 | Test Loss: 0.0342 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 285, saving checkpoint...\n",
      "Epoch 286 | Train Loss: 0.0402 | Test Loss: 0.0342 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 286, saving checkpoint...\n",
      "Epoch 287 | Train Loss: 0.0399 | Test Loss: 0.0341 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 287, saving checkpoint...\n",
      "Epoch 288 | Train Loss: 0.0398 | Test Loss: 0.0340 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 288, saving checkpoint...\n",
      "Epoch 289 | Train Loss: 0.0403 | Test Loss: 0.0339 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 289, saving checkpoint...\n",
      "Epoch 290 | Train Loss: 0.0399 | Test Loss: 0.0338 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 290, saving checkpoint...\n",
      "Epoch 291 | Train Loss: 0.0401 | Test Loss: 0.0338 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 291, saving checkpoint...\n",
      "Epoch 292 | Train Loss: 0.0395 | Test Loss: 0.0337 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 292, saving checkpoint...\n",
      "Epoch 293 | Train Loss: 0.0396 | Test Loss: 0.0336 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 293, saving checkpoint...\n",
      "Epoch 294 | Train Loss: 0.0395 | Test Loss: 0.0336 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 294, saving checkpoint...\n",
      "Epoch 295 | Train Loss: 0.0395 | Test Loss: 0.0335 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 295, saving checkpoint...\n",
      "Epoch 296 | Train Loss: 0.0392 | Test Loss: 0.0334 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 296, saving checkpoint...\n",
      "Epoch 297 | Train Loss: 0.0386 | Test Loss: 0.0333 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 297, saving checkpoint...\n",
      "Epoch 298 | Train Loss: 0.0391 | Test Loss: 0.0332 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 298, saving checkpoint...\n",
      "Epoch 299 | Train Loss: 0.0389 | Test Loss: 0.0331 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 299, saving checkpoint...\n",
      "Epoch 300 | Train Loss: 0.0386 | Test Loss: 0.0331 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 300, saving checkpoint...\n",
      "Epoch 301 | Train Loss: 0.0386 | Test Loss: 0.0330 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 301, saving checkpoint...\n",
      "Epoch 302 | Train Loss: 0.0385 | Test Loss: 0.0329 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 302, saving checkpoint...\n",
      "Epoch 303 | Train Loss: 0.0386 | Test Loss: 0.0329 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 303, saving checkpoint...\n",
      "Epoch 304 | Train Loss: 0.0385 | Test Loss: 0.0328 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 304, saving checkpoint...\n",
      "Epoch 305 | Train Loss: 0.0381 | Test Loss: 0.0327 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 305, saving checkpoint...\n",
      "Epoch 306 | Train Loss: 0.0383 | Test Loss: 0.0327 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 306, saving checkpoint...\n",
      "Epoch 307 | Train Loss: 0.0382 | Test Loss: 0.0326 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 307, saving checkpoint...\n",
      "Epoch 308 | Train Loss: 0.0382 | Test Loss: 0.0325 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 308, saving checkpoint...\n",
      "Epoch 309 | Train Loss: 0.0384 | Test Loss: 0.0325 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 309, saving checkpoint...\n",
      "Epoch 310 | Train Loss: 0.0375 | Test Loss: 0.0324 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 310, saving checkpoint...\n",
      "Epoch 311 | Train Loss: 0.0379 | Test Loss: 0.0323 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 311, saving checkpoint...\n",
      "Epoch 312 | Train Loss: 0.0379 | Test Loss: 0.0323 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 312, saving checkpoint...\n",
      "Epoch 313 | Train Loss: 0.0377 | Test Loss: 0.0322 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 313, saving checkpoint...\n",
      "Epoch 314 | Train Loss: 0.0381 | Test Loss: 0.0321 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 314, saving checkpoint...\n",
      "Epoch 315 | Train Loss: 0.0375 | Test Loss: 0.0321 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 315, saving checkpoint...\n",
      "Epoch 316 | Train Loss: 0.0376 | Test Loss: 0.0320 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 316, saving checkpoint...\n",
      "Epoch 317 | Train Loss: 0.0373 | Test Loss: 0.0319 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 317, saving checkpoint...\n",
      "Epoch 318 | Train Loss: 0.0371 | Test Loss: 0.0319 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 318, saving checkpoint...\n",
      "Epoch 319 | Train Loss: 0.0373 | Test Loss: 0.0318 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 319, saving checkpoint...\n",
      "Epoch 320 | Train Loss: 0.0371 | Test Loss: 0.0317 | LR: 0.001000 | Time: 0.10 sec\n",
      "New best model found at epoch 320, saving checkpoint...\n",
      "Epoch 321 | Train Loss: 0.0372 | Test Loss: 0.0316 | LR: 0.001000 | Time: 0.11 sec\n",
      "New best model found at epoch 321, saving checkpoint...\n",
      "Epoch 322 | Train Loss: 0.0375 | Test Loss: 0.0316 | LR: 0.001000 | Time: 0.11 sec\n",
      "New best model found at epoch 322, saving checkpoint...\n",
      "Epoch 323 | Train Loss: 0.0368 | Test Loss: 0.0316 | LR: 0.001000 | Time: 0.10 sec\n",
      "New best model found at epoch 323, saving checkpoint...\n",
      "Epoch 324 | Train Loss: 0.0371 | Test Loss: 0.0315 | LR: 0.001000 | Time: 0.10 sec\n",
      "New best model found at epoch 324, saving checkpoint...\n",
      "Epoch 325 | Train Loss: 0.0369 | Test Loss: 0.0314 | LR: 0.001000 | Time: 0.10 sec\n",
      "New best model found at epoch 325, saving checkpoint...\n",
      "Epoch 326 | Train Loss: 0.0368 | Test Loss: 0.0314 | LR: 0.001000 | Time: 0.10 sec\n",
      "New best model found at epoch 326, saving checkpoint...\n",
      "Epoch 327 | Train Loss: 0.0367 | Test Loss: 0.0313 | LR: 0.001000 | Time: 0.11 sec\n",
      "New best model found at epoch 327, saving checkpoint...\n",
      "Epoch 328 | Train Loss: 0.0368 | Test Loss: 0.0312 | LR: 0.001000 | Time: 0.11 sec\n",
      "New best model found at epoch 328, saving checkpoint...\n",
      "Epoch 329 | Train Loss: 0.0365 | Test Loss: 0.0312 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 329, saving checkpoint...\n",
      "Epoch 330 | Train Loss: 0.0362 | Test Loss: 0.0311 | LR: 0.001000 | Time: 0.11 sec\n",
      "New best model found at epoch 330, saving checkpoint...\n",
      "Epoch 331 | Train Loss: 0.0366 | Test Loss: 0.0311 | LR: 0.001000 | Time: 0.11 sec\n",
      "New best model found at epoch 331, saving checkpoint...\n",
      "Epoch 332 | Train Loss: 0.0364 | Test Loss: 0.0310 | LR: 0.001000 | Time: 0.11 sec\n",
      "New best model found at epoch 332, saving checkpoint...\n",
      "Epoch 333 | Train Loss: 0.0363 | Test Loss: 0.0310 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 333, saving checkpoint...\n",
      "Epoch 334 | Train Loss: 0.0362 | Test Loss: 0.0309 | LR: 0.001000 | Time: 0.11 sec\n",
      "New best model found at epoch 334, saving checkpoint...\n",
      "Epoch 335 | Train Loss: 0.0365 | Test Loss: 0.0309 | LR: 0.001000 | Time: 0.11 sec\n",
      "New best model found at epoch 335, saving checkpoint...\n",
      "Epoch 336 | Train Loss: 0.0361 | Test Loss: 0.0308 | LR: 0.001000 | Time: 0.11 sec\n",
      "New best model found at epoch 336, saving checkpoint...\n",
      "Epoch 337 | Train Loss: 0.0360 | Test Loss: 0.0308 | LR: 0.001000 | Time: 0.11 sec\n",
      "New best model found at epoch 337, saving checkpoint...\n",
      "Epoch 338 | Train Loss: 0.0361 | Test Loss: 0.0307 | LR: 0.001000 | Time: 0.11 sec\n",
      "New best model found at epoch 338, saving checkpoint...\n",
      "Epoch 339 | Train Loss: 0.0362 | Test Loss: 0.0307 | LR: 0.001000 | Time: 0.10 sec\n",
      "New best model found at epoch 339, saving checkpoint...\n",
      "Epoch 340 | Train Loss: 0.0356 | Test Loss: 0.0306 | LR: 0.001000 | Time: 0.11 sec\n",
      "New best model found at epoch 340, saving checkpoint...\n",
      "Epoch 341 | Train Loss: 0.0358 | Test Loss: 0.0305 | LR: 0.001000 | Time: 0.10 sec\n",
      "New best model found at epoch 341, saving checkpoint...\n",
      "Epoch 342 | Train Loss: 0.0357 | Test Loss: 0.0305 | LR: 0.001000 | Time: 0.11 sec\n",
      "New best model found at epoch 342, saving checkpoint...\n",
      "Epoch 343 | Train Loss: 0.0356 | Test Loss: 0.0304 | LR: 0.001000 | Time: 0.10 sec\n",
      "New best model found at epoch 343, saving checkpoint...\n",
      "Epoch 344 | Train Loss: 0.0354 | Test Loss: 0.0304 | LR: 0.001000 | Time: 0.11 sec\n",
      "New best model found at epoch 344, saving checkpoint...\n",
      "Epoch 345 | Train Loss: 0.0354 | Test Loss: 0.0303 | LR: 0.001000 | Time: 0.10 sec\n",
      "New best model found at epoch 345, saving checkpoint...\n",
      "Epoch 346 | Train Loss: 0.0354 | Test Loss: 0.0302 | LR: 0.001000 | Time: 0.11 sec\n",
      "New best model found at epoch 346, saving checkpoint...\n",
      "Epoch 347 | Train Loss: 0.0352 | Test Loss: 0.0302 | LR: 0.001000 | Time: 0.10 sec\n",
      "New best model found at epoch 347, saving checkpoint...\n",
      "Epoch 348 | Train Loss: 0.0356 | Test Loss: 0.0301 | LR: 0.001000 | Time: 0.10 sec\n",
      "New best model found at epoch 348, saving checkpoint...\n",
      "Epoch 349 | Train Loss: 0.0352 | Test Loss: 0.0300 | LR: 0.001000 | Time: 0.10 sec\n",
      "New best model found at epoch 349, saving checkpoint...\n",
      "Epoch 350 | Train Loss: 0.0355 | Test Loss: 0.0300 | LR: 0.001000 | Time: 0.10 sec\n",
      "New best model found at epoch 350, saving checkpoint...\n",
      "Epoch 351 | Train Loss: 0.0353 | Test Loss: 0.0299 | LR: 0.001000 | Time: 0.11 sec\n",
      "New best model found at epoch 351, saving checkpoint...\n",
      "Epoch 352 | Train Loss: 0.0350 | Test Loss: 0.0299 | LR: 0.001000 | Time: 0.10 sec\n",
      "New best model found at epoch 352, saving checkpoint...\n",
      "Epoch 353 | Train Loss: 0.0351 | Test Loss: 0.0298 | LR: 0.001000 | Time: 0.67 sec\n",
      "New best model found at epoch 353, saving checkpoint...\n",
      "Epoch 354 | Train Loss: 0.0352 | Test Loss: 0.0298 | LR: 0.001000 | Time: 0.11 sec\n",
      "New best model found at epoch 354, saving checkpoint...\n",
      "Epoch 355 | Train Loss: 0.0352 | Test Loss: 0.0297 | LR: 0.001000 | Time: 0.10 sec\n",
      "New best model found at epoch 355, saving checkpoint...\n",
      "Epoch 356 | Train Loss: 0.0349 | Test Loss: 0.0297 | LR: 0.001000 | Time: 0.10 sec\n",
      "New best model found at epoch 356, saving checkpoint...\n",
      "Epoch 357 | Train Loss: 0.0349 | Test Loss: 0.0296 | LR: 0.001000 | Time: 0.10 sec\n",
      "New best model found at epoch 357, saving checkpoint...\n",
      "Epoch 358 | Train Loss: 0.0348 | Test Loss: 0.0295 | LR: 0.001000 | Time: 0.11 sec\n",
      "New best model found at epoch 358, saving checkpoint...\n",
      "Epoch 359 | Train Loss: 0.0346 | Test Loss: 0.0295 | LR: 0.001000 | Time: 0.11 sec\n",
      "New best model found at epoch 359, saving checkpoint...\n",
      "Epoch 360 | Train Loss: 0.0345 | Test Loss: 0.0294 | LR: 0.001000 | Time: 0.10 sec\n",
      "New best model found at epoch 360, saving checkpoint...\n",
      "Epoch 361 | Train Loss: 0.0348 | Test Loss: 0.0294 | LR: 0.001000 | Time: 0.10 sec\n",
      "New best model found at epoch 361, saving checkpoint...\n",
      "Epoch 362 | Train Loss: 0.0352 | Test Loss: 0.0293 | LR: 0.001000 | Time: 0.10 sec\n",
      "New best model found at epoch 362, saving checkpoint...\n",
      "Epoch 363 | Train Loss: 0.0347 | Test Loss: 0.0293 | LR: 0.001000 | Time: 0.10 sec\n",
      "New best model found at epoch 363, saving checkpoint...\n",
      "Epoch 364 | Train Loss: 0.0345 | Test Loss: 0.0292 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 364, saving checkpoint...\n",
      "Epoch 365 | Train Loss: 0.0347 | Test Loss: 0.0292 | LR: 0.001000 | Time: 0.11 sec\n",
      "New best model found at epoch 365, saving checkpoint...\n",
      "Epoch 366 | Train Loss: 0.0346 | Test Loss: 0.0291 | LR: 0.001000 | Time: 0.11 sec\n",
      "New best model found at epoch 366, saving checkpoint...\n",
      "Epoch 367 | Train Loss: 0.0338 | Test Loss: 0.0291 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 367, saving checkpoint...\n",
      "Epoch 368 | Train Loss: 0.0347 | Test Loss: 0.0290 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 368, saving checkpoint...\n",
      "Epoch 369 | Train Loss: 0.0344 | Test Loss: 0.0290 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 369, saving checkpoint...\n",
      "Epoch 370 | Train Loss: 0.0341 | Test Loss: 0.0289 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 370, saving checkpoint...\n",
      "Epoch 371 | Train Loss: 0.0342 | Test Loss: 0.0289 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 371, saving checkpoint...\n",
      "Epoch 372 | Train Loss: 0.0339 | Test Loss: 0.0289 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 372, saving checkpoint...\n",
      "Epoch 373 | Train Loss: 0.0342 | Test Loss: 0.0288 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 373, saving checkpoint...\n",
      "Epoch 374 | Train Loss: 0.0341 | Test Loss: 0.0287 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 374, saving checkpoint...\n",
      "Epoch 375 | Train Loss: 0.0339 | Test Loss: 0.0287 | LR: 0.001000 | Time: 0.14 sec\n",
      "New best model found at epoch 375, saving checkpoint...\n",
      "Epoch 376 | Train Loss: 0.0342 | Test Loss: 0.0286 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 376, saving checkpoint...\n",
      "Epoch 377 | Train Loss: 0.0337 | Test Loss: 0.0286 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 377, saving checkpoint...\n",
      "Epoch 378 | Train Loss: 0.0338 | Test Loss: 0.0285 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 378, saving checkpoint...\n",
      "Epoch 379 | Train Loss: 0.0335 | Test Loss: 0.0285 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 379, saving checkpoint...\n",
      "Epoch 380 | Train Loss: 0.0337 | Test Loss: 0.0284 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 380, saving checkpoint...\n",
      "Epoch 381 | Train Loss: 0.0333 | Test Loss: 0.0284 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 381, saving checkpoint...\n",
      "Epoch 382 | Train Loss: 0.0335 | Test Loss: 0.0283 | LR: 0.001000 | Time: 0.15 sec\n",
      "New best model found at epoch 382, saving checkpoint...\n",
      "Epoch 383 | Train Loss: 0.0334 | Test Loss: 0.0283 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 383, saving checkpoint...\n",
      "Epoch 384 | Train Loss: 0.0334 | Test Loss: 0.0282 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 384, saving checkpoint...\n",
      "Epoch 385 | Train Loss: 0.0333 | Test Loss: 0.0282 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 385, saving checkpoint...\n",
      "Epoch 386 | Train Loss: 0.0334 | Test Loss: 0.0282 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 386, saving checkpoint...\n",
      "Epoch 387 | Train Loss: 0.0332 | Test Loss: 0.0281 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 387, saving checkpoint...\n",
      "Epoch 388 | Train Loss: 0.0332 | Test Loss: 0.0281 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 388, saving checkpoint...\n",
      "Epoch 389 | Train Loss: 0.0333 | Test Loss: 0.0280 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 389, saving checkpoint...\n",
      "Epoch 390 | Train Loss: 0.0333 | Test Loss: 0.0280 | LR: 0.001000 | Time: 0.14 sec\n",
      "New best model found at epoch 390, saving checkpoint...\n",
      "Epoch 391 | Train Loss: 0.0331 | Test Loss: 0.0280 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 391, saving checkpoint...\n",
      "Epoch 392 | Train Loss: 0.0334 | Test Loss: 0.0279 | LR: 0.001000 | Time: 0.11 sec\n",
      "New best model found at epoch 392, saving checkpoint...\n",
      "Epoch 393 | Train Loss: 0.0332 | Test Loss: 0.0279 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 393, saving checkpoint...\n",
      "Epoch 394 | Train Loss: 0.0329 | Test Loss: 0.0278 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 394, saving checkpoint...\n",
      "Epoch 395 | Train Loss: 0.0334 | Test Loss: 0.0278 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 395, saving checkpoint...\n",
      "Epoch 396 | Train Loss: 0.0328 | Test Loss: 0.0278 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 396, saving checkpoint...\n",
      "Epoch 397 | Train Loss: 0.0328 | Test Loss: 0.0277 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 397, saving checkpoint...\n",
      "Epoch 398 | Train Loss: 0.0331 | Test Loss: 0.0277 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 398, saving checkpoint...\n",
      "Epoch 399 | Train Loss: 0.0329 | Test Loss: 0.0276 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 399, saving checkpoint...\n",
      "Epoch 400 | Train Loss: 0.0326 | Test Loss: 0.0276 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 400, saving checkpoint...\n",
      "Epoch 401 | Train Loss: 0.0326 | Test Loss: 0.0276 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 401, saving checkpoint...\n",
      "Epoch 402 | Train Loss: 0.0322 | Test Loss: 0.0275 | LR: 0.001000 | Time: 0.14 sec\n",
      "New best model found at epoch 402, saving checkpoint...\n",
      "Epoch 403 | Train Loss: 0.0324 | Test Loss: 0.0275 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 403, saving checkpoint...\n",
      "Epoch 404 | Train Loss: 0.0329 | Test Loss: 0.0274 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 404, saving checkpoint...\n",
      "Epoch 405 | Train Loss: 0.0327 | Test Loss: 0.0274 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 405, saving checkpoint...\n",
      "Epoch 406 | Train Loss: 0.0327 | Test Loss: 0.0273 | LR: 0.001000 | Time: 0.14 sec\n",
      "New best model found at epoch 406, saving checkpoint...\n",
      "Epoch 407 | Train Loss: 0.0327 | Test Loss: 0.0273 | LR: 0.001000 | Time: 0.14 sec\n",
      "New best model found at epoch 407, saving checkpoint...\n",
      "Epoch 408 | Train Loss: 0.0330 | Test Loss: 0.0273 | LR: 0.001000 | Time: 0.16 sec\n",
      "New best model found at epoch 408, saving checkpoint...\n",
      "Epoch 409 | Train Loss: 0.0324 | Test Loss: 0.0273 | LR: 0.001000 | Time: 0.16 sec\n",
      "New best model found at epoch 409, saving checkpoint...\n",
      "Epoch 410 | Train Loss: 0.0321 | Test Loss: 0.0272 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 410, saving checkpoint...\n",
      "Epoch 411 | Train Loss: 0.0326 | Test Loss: 0.0272 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 411, saving checkpoint...\n",
      "Epoch 412 | Train Loss: 0.0326 | Test Loss: 0.0271 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 412, saving checkpoint...\n",
      "Epoch 413 | Train Loss: 0.0321 | Test Loss: 0.0271 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 413, saving checkpoint...\n",
      "Epoch 414 | Train Loss: 0.0324 | Test Loss: 0.0271 | LR: 0.001000 | Time: 0.16 sec\n",
      "New best model found at epoch 414, saving checkpoint...\n",
      "Epoch 415 | Train Loss: 0.0320 | Test Loss: 0.0270 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 415, saving checkpoint...\n",
      "Epoch 416 | Train Loss: 0.0320 | Test Loss: 0.0270 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 416, saving checkpoint...\n",
      "Epoch 417 | Train Loss: 0.0320 | Test Loss: 0.0269 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 417, saving checkpoint...\n",
      "Epoch 418 | Train Loss: 0.0319 | Test Loss: 0.0269 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 418, saving checkpoint...\n",
      "Epoch 419 | Train Loss: 0.0319 | Test Loss: 0.0268 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 419, saving checkpoint...\n",
      "Epoch 420 | Train Loss: 0.0317 | Test Loss: 0.0268 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 420, saving checkpoint...\n",
      "Epoch 421 | Train Loss: 0.0320 | Test Loss: 0.0268 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 421, saving checkpoint...\n",
      "Epoch 422 | Train Loss: 0.0320 | Test Loss: 0.0267 | LR: 0.001000 | Time: 0.19 sec\n",
      "New best model found at epoch 422, saving checkpoint...\n",
      "Epoch 423 | Train Loss: 0.0321 | Test Loss: 0.0267 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 423, saving checkpoint...\n",
      "Epoch 424 | Train Loss: 0.0314 | Test Loss: 0.0267 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 424, saving checkpoint...\n",
      "Epoch 425 | Train Loss: 0.0321 | Test Loss: 0.0266 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 425, saving checkpoint...\n",
      "Epoch 426 | Train Loss: 0.0317 | Test Loss: 0.0266 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 426, saving checkpoint...\n",
      "Epoch 427 | Train Loss: 0.0319 | Test Loss: 0.0266 | LR: 0.001000 | Time: 0.15 sec\n",
      "New best model found at epoch 427, saving checkpoint...\n",
      "Epoch 428 | Train Loss: 0.0317 | Test Loss: 0.0265 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 428, saving checkpoint...\n",
      "Epoch 429 | Train Loss: 0.0318 | Test Loss: 0.0265 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 429, saving checkpoint...\n",
      "Epoch 430 | Train Loss: 0.0315 | Test Loss: 0.0265 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 430, saving checkpoint...\n",
      "Epoch 431 | Train Loss: 0.0317 | Test Loss: 0.0264 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 431, saving checkpoint...\n",
      "Epoch 432 | Train Loss: 0.0315 | Test Loss: 0.0264 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 432, saving checkpoint...\n",
      "Epoch 433 | Train Loss: 0.0311 | Test Loss: 0.0263 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 433, saving checkpoint...\n",
      "Epoch 434 | Train Loss: 0.0314 | Test Loss: 0.0263 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 434, saving checkpoint...\n",
      "Epoch 435 | Train Loss: 0.0315 | Test Loss: 0.0263 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 435, saving checkpoint...\n",
      "Epoch 436 | Train Loss: 0.0318 | Test Loss: 0.0262 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 436, saving checkpoint...\n",
      "Epoch 437 | Train Loss: 0.0313 | Test Loss: 0.0262 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 437, saving checkpoint...\n",
      "Epoch 438 | Train Loss: 0.0311 | Test Loss: 0.0262 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 438, saving checkpoint...\n",
      "Epoch 439 | Train Loss: 0.0311 | Test Loss: 0.0261 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 439, saving checkpoint...\n",
      "Epoch 440 | Train Loss: 0.0308 | Test Loss: 0.0261 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 440, saving checkpoint...\n",
      "Epoch 441 | Train Loss: 0.0315 | Test Loss: 0.0261 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 441, saving checkpoint...\n",
      "Epoch 442 | Train Loss: 0.0313 | Test Loss: 0.0261 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 442, saving checkpoint...\n",
      "Epoch 443 | Train Loss: 0.0309 | Test Loss: 0.0260 | LR: 0.001000 | Time: 0.14 sec\n",
      "New best model found at epoch 443, saving checkpoint...\n",
      "Epoch 444 | Train Loss: 0.0308 | Test Loss: 0.0260 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 444, saving checkpoint...\n",
      "Epoch 445 | Train Loss: 0.0310 | Test Loss: 0.0260 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 445, saving checkpoint...\n",
      "Epoch 446 | Train Loss: 0.0308 | Test Loss: 0.0260 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 446, saving checkpoint...\n",
      "Epoch 447 | Train Loss: 0.0308 | Test Loss: 0.0259 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 447, saving checkpoint...\n",
      "Epoch 448 | Train Loss: 0.0311 | Test Loss: 0.0259 | LR: 0.001000 | Time: 0.14 sec\n",
      "New best model found at epoch 448, saving checkpoint...\n",
      "Epoch 449 | Train Loss: 0.0306 | Test Loss: 0.0259 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 449, saving checkpoint...\n",
      "Epoch 450 | Train Loss: 0.0311 | Test Loss: 0.0258 | LR: 0.001000 | Time: 0.14 sec\n",
      "New best model found at epoch 450, saving checkpoint...\n",
      "Epoch 451 | Train Loss: 0.0310 | Test Loss: 0.0258 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 451, saving checkpoint...\n",
      "Epoch 452 | Train Loss: 0.0309 | Test Loss: 0.0258 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 452, saving checkpoint...\n",
      "Epoch 453 | Train Loss: 0.0308 | Test Loss: 0.0258 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 453, saving checkpoint...\n",
      "Epoch 454 | Train Loss: 0.0307 | Test Loss: 0.0257 | LR: 0.001000 | Time: 0.14 sec\n",
      "New best model found at epoch 454, saving checkpoint...\n",
      "Epoch 455 | Train Loss: 0.0304 | Test Loss: 0.0257 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 455, saving checkpoint...\n",
      "Epoch 456 | Train Loss: 0.0307 | Test Loss: 0.0257 | LR: 0.001000 | Time: 0.14 sec\n",
      "New best model found at epoch 456, saving checkpoint...\n",
      "Epoch 457 | Train Loss: 0.0307 | Test Loss: 0.0256 | LR: 0.001000 | Time: 0.16 sec\n",
      "New best model found at epoch 457, saving checkpoint...\n",
      "Epoch 458 | Train Loss: 0.0303 | Test Loss: 0.0256 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 458, saving checkpoint...\n",
      "Epoch 459 | Train Loss: 0.0305 | Test Loss: 0.0256 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 459, saving checkpoint...\n",
      "Epoch 460 | Train Loss: 0.0308 | Test Loss: 0.0256 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 460, saving checkpoint...\n",
      "Epoch 461 | Train Loss: 0.0305 | Test Loss: 0.0255 | LR: 0.001000 | Time: 0.15 sec\n",
      "New best model found at epoch 461, saving checkpoint...\n",
      "Epoch 462 | Train Loss: 0.0305 | Test Loss: 0.0255 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 462, saving checkpoint...\n",
      "Epoch 463 | Train Loss: 0.0306 | Test Loss: 0.0255 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 463, saving checkpoint...\n",
      "Epoch 464 | Train Loss: 0.0305 | Test Loss: 0.0255 | LR: 0.001000 | Time: 0.15 sec\n",
      "New best model found at epoch 464, saving checkpoint...\n",
      "Epoch 465 | Train Loss: 0.0306 | Test Loss: 0.0254 | LR: 0.001000 | Time: 0.14 sec\n",
      "New best model found at epoch 465, saving checkpoint...\n",
      "Epoch 466 | Train Loss: 0.0304 | Test Loss: 0.0254 | LR: 0.001000 | Time: 0.16 sec\n",
      "New best model found at epoch 466, saving checkpoint...\n",
      "Epoch 467 | Train Loss: 0.0303 | Test Loss: 0.0253 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 467, saving checkpoint...\n",
      "Epoch 468 | Train Loss: 0.0300 | Test Loss: 0.0253 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 468, saving checkpoint...\n",
      "Epoch 469 | Train Loss: 0.0302 | Test Loss: 0.0253 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 469, saving checkpoint...\n",
      "Epoch 470 | Train Loss: 0.0301 | Test Loss: 0.0253 | LR: 0.001000 | Time: 0.15 sec\n",
      "New best model found at epoch 470, saving checkpoint...\n",
      "Epoch 471 | Train Loss: 0.0300 | Test Loss: 0.0253 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 471, saving checkpoint...\n",
      "Epoch 472 | Train Loss: 0.0300 | Test Loss: 0.0252 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 472, saving checkpoint...\n",
      "Epoch 473 | Train Loss: 0.0301 | Test Loss: 0.0252 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 473, saving checkpoint...\n",
      "Epoch 474 | Train Loss: 0.0300 | Test Loss: 0.0252 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 474, saving checkpoint...\n",
      "Epoch 475 | Train Loss: 0.0302 | Test Loss: 0.0252 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 475, saving checkpoint...\n",
      "Epoch 476 | Train Loss: 0.0302 | Test Loss: 0.0252 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 476, saving checkpoint...\n",
      "Epoch 477 | Train Loss: 0.0306 | Test Loss: 0.0251 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 477, saving checkpoint...\n",
      "Epoch 478 | Train Loss: 0.0302 | Test Loss: 0.0251 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 478, saving checkpoint...\n",
      "Epoch 479 | Train Loss: 0.0297 | Test Loss: 0.0250 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 479, saving checkpoint...\n",
      "Epoch 480 | Train Loss: 0.0301 | Test Loss: 0.0250 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 480, saving checkpoint...\n",
      "Epoch 481 | Train Loss: 0.0298 | Test Loss: 0.0250 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 481, saving checkpoint...\n",
      "Epoch 482 | Train Loss: 0.0298 | Test Loss: 0.0250 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 482, saving checkpoint...\n",
      "Epoch 483 | Train Loss: 0.0299 | Test Loss: 0.0249 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 483, saving checkpoint...\n",
      "Epoch 484 | Train Loss: 0.0300 | Test Loss: 0.0249 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 484, saving checkpoint...\n",
      "Epoch 485 | Train Loss: 0.0295 | Test Loss: 0.0249 | LR: 0.001000 | Time: 0.16 sec\n",
      "New best model found at epoch 485, saving checkpoint...\n",
      "Epoch 486 | Train Loss: 0.0296 | Test Loss: 0.0249 | LR: 0.001000 | Time: 0.14 sec\n",
      "New best model found at epoch 486, saving checkpoint...\n",
      "Epoch 487 | Train Loss: 0.0300 | Test Loss: 0.0249 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 487, saving checkpoint...\n",
      "Epoch 488 | Train Loss: 0.0301 | Test Loss: 0.0249 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 488, saving checkpoint...\n",
      "Epoch 489 | Train Loss: 0.0299 | Test Loss: 0.0248 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 489, saving checkpoint...\n",
      "Epoch 490 | Train Loss: 0.0295 | Test Loss: 0.0248 | LR: 0.001000 | Time: 0.14 sec\n",
      "New best model found at epoch 490, saving checkpoint...\n",
      "Epoch 491 | Train Loss: 0.0297 | Test Loss: 0.0248 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 491, saving checkpoint...\n",
      "Epoch 492 | Train Loss: 0.0295 | Test Loss: 0.0248 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 492, saving checkpoint...\n",
      "Epoch 493 | Train Loss: 0.0297 | Test Loss: 0.0248 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 493, saving checkpoint...\n",
      "Epoch 494 | Train Loss: 0.0297 | Test Loss: 0.0247 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 494, saving checkpoint...\n",
      "Epoch 495 | Train Loss: 0.0297 | Test Loss: 0.0247 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 495, saving checkpoint...\n",
      "Epoch 496 | Train Loss: 0.0296 | Test Loss: 0.0247 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 496, saving checkpoint...\n",
      "Epoch 497 | Train Loss: 0.0298 | Test Loss: 0.0246 | LR: 0.001000 | Time: 0.13 sec\n",
      "New best model found at epoch 497, saving checkpoint...\n",
      "Epoch 498 | Train Loss: 0.0296 | Test Loss: 0.0246 | LR: 0.001000 | Time: 0.12 sec\n",
      "New best model found at epoch 498, saving checkpoint...\n",
      "Epoch 499 | Train Loss: 0.0295 | Test Loss: 0.0246 | LR: 0.001000 | Time: 0.15 sec\n",
      "New best model found at epoch 499, saving checkpoint...\n",
      "Epoch 500 | Train Loss: 0.0295 | Test Loss: 0.0246 | LR: 0.001000 | Time: 0.14 sec\n",
      "New best model found at epoch 500, saving checkpoint...\n"
     ]
    }
   ],
   "source": [
    "model = TitanicAutoencoder(input_dim=X_train.shape[1]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5) \n",
    "# This will halve the learning rate if avg_test_loss doesn't improve for 5 epochs.\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Create a timestamped run directory\n",
    "RUN_NAME = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "LOG_DIR = f\"runs/autoencoder_{RUN_NAME}\" # Create directories for logs and checkpoints\n",
    "CHECKPOINT_DIR = f\"checkpoints_{RUN_NAME}\" # Create directories for logs and checkpoints\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True) # Create directories if they don't exis\n",
    "BATCH_SIZE = 256 # Adjust batch size as needed\n",
    "WORKERS = 1 # Number of workers for DataLoader, adjust based on your system\n",
    "\n",
    "writer = SummaryWriter(log_dir=LOG_DIR)\n",
    "\n",
    "#Training loop\n",
    "# Prepare training and test datasets\n",
    "train_ds = TitanicAutoencoderDataset(X_train)\n",
    "test_ds = TitanicAutoencoderDataset(X_test)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, num_workers=WORKERS, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, num_workers=WORKERS)\n",
    "\n",
    "# Track losses\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "best_loss = float('inf') # Initialize best_loss for comparison and checkpointing\n",
    "# Training loop\n",
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        recon = model(x)\n",
    "        loss = loss_fn(recon, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_duration = end_time - start_time\n",
    "    # GPU metrics\n",
    "    gpu_alloc = torch.cuda.memory_allocated() / 1024**2\n",
    "    gpu_reserved = torch.cuda.memory_reserved() / 1024**2\n",
    "    writer.add_scalar(\"GPU/Memory_Allocated_MB\", gpu_alloc, epoch)\n",
    "    writer.add_scalar(\"GPU/Memory_Reserved_MB\", gpu_reserved, epoch)\n",
    "\n",
    "    try:\n",
    "        gpu_util, mem_used = get_gpu_utilization()\n",
    "        writer.add_scalar(\"GPU/Utilization_%\", gpu_util, epoch)\n",
    "        writer.add_scalar(\"GPU/Memory_Used_MB\", mem_used, epoch)\n",
    "    except:\n",
    "        pass  # In case nvidia-smi isn't available\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            recon = model(x)\n",
    "            loss = loss_fn(recon, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    test_losses.append(avg_test_loss)\n",
    "\n",
    "    # âœ… Log scalar losses\n",
    "    writer.add_scalar(\"Loss/train\", avg_train_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/test\", avg_test_loss, epoch)\n",
    "    writer.add_figure(\"Loss Overlap Curve\", plot_loss_curve(train_losses, test_losses), global_step=epoch)\n",
    "\n",
    "    # âœ… Log weights and gradients\n",
    "    for name, param in model.named_parameters():\n",
    "        writer.add_histogram(f\"Weights/{name}\", param, epoch)\n",
    "        if param.grad is not None:\n",
    "            writer.add_histogram(f\"Gradients/{name}\", param.grad, epoch)\n",
    "\n",
    "    # âœ… Log activation from encoder\n",
    "    with torch.no_grad():\n",
    "        activation_sample = torch.tensor(X_train[:1], dtype=torch.float32).to(device)\n",
    "        encoded = model.encoder(activation_sample)\n",
    "        writer.add_histogram(\"Activations/EncoderOutput\", encoded, epoch)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        writer.add_scalar(\"LR\", current_lr, epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:2d} | Train Loss: {avg_train_loss:.4f} | Test Loss: {avg_test_loss:.4f} | LR: {current_lr:.6f} | Time: {epoch_duration:.2f} sec\")\n",
    "\n",
    "    # Save only the best model based on test loss\n",
    "    if avg_test_loss < best_loss:\n",
    "        best_loss = avg_test_loss\n",
    "        print(f\"New best model found at epoch {epoch+1}, saving checkpoint...\")\n",
    "        \n",
    "            # Save the model state\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_losses': train_losses,\n",
    "            'test_losses': test_losses,\n",
    "            }, f\"{CHECKPOINT_DIR}/autoencoder_epoch{epoch+1}.pt\")\n",
    "    \n",
    "    scheduler.step(avg_test_loss) # Adjust learning rate based on test loss\n",
    "\n",
    "# âœ… After training â€” log embeddings to projector\n",
    "with torch.no_grad():\n",
    "    sample_input = torch.tensor(X_test[:500], dtype=torch.float32).to(device)\n",
    "    latent_vectors = model.encoder(sample_input)\n",
    "    metadata = [f\"Passenger {i}\" for i in range(sample_input.shape[0])]\n",
    "    writer.add_embedding(latent_vectors, metadata=metadata, tag=\"LatentEmbeddings\", global_step=num_epochs)\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333375cc",
   "metadata": {},
   "source": [
    "## Run Tensorboard to inspect results\n",
    "\n",
    "To run Tensorboard, you may run it from the same directory as this notebook. You may run the following command either during or after the training run:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "Make sure you activated the appropriate virtual environment which has tensorboard installed.\n",
    "\n",
    "You should see the following:\n",
    "\n",
    "```\n",
    "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
    "TensorBoard 2.20.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
    "```\n",
    "\n",
    "and you may use a browser to access tensorboard via the URL above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9669f711",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06e0b7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (MSE)\")\n",
    "plt.title(\"Training and Test Loss Over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59e27fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.savefig(\"loss_curve.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41dff05",
   "metadata": {},
   "source": [
    "# Get embedding vector from encoder side of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7683326f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    embeddings = model.encoder(x_tensor)  # Shape: [n_samples, latent_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43970c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([178, 4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8171ad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_df = pd.DataFrame(embeddings.cpu().numpy(), columns=[f\"embeddings{i}\" for i in range(embeddings.shape[1])])\n",
    "embedding_df.to_csv(\"titanic_latent_embeddings.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70e6181e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## More later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579c6a71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (recmd_venv)",
   "language": "python",
   "name": "recmd_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
