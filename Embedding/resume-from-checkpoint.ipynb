{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d155084",
   "metadata": {},
   "source": [
    "## Load model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5220147b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5504c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = \"checkpoints_2025-07-31_21-51-51/autoencoder_epoch500.pt\"  # Example checkpoint directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b440275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(path, model, optimizer, scheduler, device):\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"‚ùå Checkpoint not found: {path}\")\n",
    "        return 0, [], []\n",
    "\n",
    "    print(f\"üîÑ Loading checkpoint from: {path}\")\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    if 'scheduler_state_dict' in checkpoint:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "    epoch = checkpoint.get('epoch', 0) + 1\n",
    "    train_losses = checkpoint.get('train_losses', [])\n",
    "    test_losses = checkpoint.get('test_losses', [])\n",
    "\n",
    "    print(f\"‚úÖ Resumed from epoch {epoch}\")\n",
    "    return epoch, train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be1fce2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=4):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)       # compressed embedding\n",
    "        x_recon = self.decoder(z) # reconstructed input\n",
    "        return x_recon\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)    # get embedding only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2305fe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 10\n",
    "LATENT_DIM = 8\n",
    "model = TitanicAutoencoder(input_dim=INPUT_DIM, latent_dim=LATENT_DIM).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa4b1b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading checkpoint from: checkpoints_2025-07-31_21-51-51/autoencoder_epoch500.pt\n",
      "‚úÖ Resumed from epoch 500\n"
     ]
    }
   ],
   "source": [
    "# Load the checkpoint\n",
    "start_epoch, train_losses, test_losses = load_checkpoint(\n",
    "    CHECKPOINT_PATH, model, optimizer, scheduler, device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27b05ef",
   "metadata": {},
   "source": [
    "## Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81043d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sns.load_dataset(\"titanic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2efc732c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training DataFrame: (713, 15)\n",
      "Shape of test DataFrame: (178, 15)\n"
     ]
    }
   ],
   "source": [
    "## Randomly select a fraction of the dataset\n",
    "FRACTION = 0.8\n",
    "\n",
    "train_df = df.sample(frac=FRACTION, random_state=42) # fix seed for reproducibility\n",
    "# Get the remaining 20% of rows for the test set\n",
    "# This is achieved by selecting rows whose index is not present in the training set\n",
    "test_df = df.drop(train_df.index)\n",
    "# Display the shapes of the resulting DataFrames\n",
    "print(f\"Shape of training DataFrame: {train_df.shape}\")\n",
    "print(f\"Shape of test DataFrame: {test_df.shape}\")\n",
    "\n",
    "# You can now save these DataFrames if needed\n",
    "train_df.to_csv('train_data.csv', index=False)\n",
    "test_df.to_csv('test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fd630dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop columns with missing value and alive column\n",
    "df_nomissing = df.drop(columns=['age', 'deck', 'embarked', 'embark_town', 'survived',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3722683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize numeric feature\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "num_cols = [\"fare\", \"sibsp\", \"parch\"]\n",
    "scaler = StandardScaler()\n",
    "df_nomissing[num_cols] = scaler.fit_transform(df_nomissing[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28e9b14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert categorial features to integers using Label Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cat_cols = [\"pclass\", \"sex\", \"class\", \"who\", \"adult_male\", \"alive\", \"alone\"]  # treat pclass as categorical\n",
    "label_encoders = {}\n",
    "\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_nomissing[col] = le.fit_transform(df_nomissing[col])\n",
    "    label_encoders[col] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a0f070e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training DataFrame: (713, 10)\n",
      "Shape of test DataFrame: (178, 10)\n"
     ]
    }
   ],
   "source": [
    "## Randomly select a fraction of the dataset\n",
    "FRACTION = 0.8\n",
    "\n",
    "train_df_nomissing = df_nomissing.sample(frac=FRACTION, random_state=42) # fix seed for reproducibility\n",
    "# Get the remaining 20% of rows for the test set\n",
    "# This is achieved by selecting rows whose index is not present in the training set\n",
    "test_df_nomissing = df_nomissing.drop(train_df_nomissing.index)\n",
    "# Display the shapes of the resulting DataFrames\n",
    "print(f\"Shape of training DataFrame: {train_df_nomissing.shape}\")\n",
    "print(f\"Shape of test DataFrame: {test_df_nomissing.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d47d8188",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df_nomissing.values.astype(\"float32\")\n",
    "X_test = test_df_nomissing.values.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4430dc37",
   "metadata": {},
   "source": [
    "## Pytorch dataset for loading data to training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a452aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TitanicAutoencoderDataset(Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.X[idx]  # input = target\n",
    "\n",
    "train_ds = TitanicAutoencoderDataset(X_train)\n",
    "test_ds = TitanicAutoencoderDataset(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b08adb4",
   "metadata": {},
   "source": [
    "## Get GPU utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38b10dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_gpu_utilization():\n",
    "    result = subprocess.run(\n",
    "        ['nvidia-smi', '--query-gpu=utilization.gpu,memory.used',\n",
    "         '--format=csv,nounits,noheader'],\n",
    "        stdout=subprocess.PIPE,\n",
    "        text=True\n",
    "    )\n",
    "    gpu_util, mem_used = map(int, result.stdout.strip().split(','))\n",
    "    return gpu_util, mem_used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa06bd7",
   "metadata": {},
   "source": [
    "## Training starts from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc015abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a timestamped run directory\n",
    "os.makedirs(\"runs-start-from-checkpoint\", exist_ok=True) # Create directories for logs and checkpoints\n",
    "RUN_NAME = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "LOG_DIR = f\"runs-checkpoint/autoencoder_{RUN_NAME}\" # Create directories for logs and checkpoints\n",
    "CHECKPOINT_DIR = f\"ckpt_checkpoints_{RUN_NAME}\" # Create directories for logs and checkpoints\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True) # Create directories if they don't exist\n",
    "BATCH_SIZE = 256 # Adjust batch size as needed\n",
    "WORKERS = 1 # Number of workers for DataLoader, adjust based on your system\n",
    "\n",
    "writer = SummaryWriter(log_dir=LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c8193f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7266f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curve(train_losses, test_losses):\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.plot(train_losses, label=\"Train Loss\", marker='o')\n",
    "    ax.plot(test_losses, label=\"Test Loss\", marker='x')\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(\"Train vs Test Loss\")\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a10236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 501 | Train Loss: 0.0011 | Test Loss: 0.0011 | LR: 0.001000 | Time: 0.09 sec\n",
      "New best model found at epoch 501, saving checkpoint...\n",
      "Epoch 502 | Train Loss: 0.0011 | Test Loss: 0.0011 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 502, saving checkpoint...\n",
      "Epoch 503 | Train Loss: 0.0011 | Test Loss: 0.0011 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 503, saving checkpoint...\n",
      "Epoch 504 | Train Loss: 0.0011 | Test Loss: 0.0011 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 504, saving checkpoint...\n",
      "Epoch 505 | Train Loss: 0.0011 | Test Loss: 0.0011 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 505, saving checkpoint...\n",
      "Epoch 506 | Train Loss: 0.0010 | Test Loss: 0.0011 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 506, saving checkpoint...\n",
      "Epoch 507 | Train Loss: 0.0011 | Test Loss: 0.0011 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 507, saving checkpoint...\n",
      "Epoch 508 | Train Loss: 0.0010 | Test Loss: 0.0011 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 508, saving checkpoint...\n",
      "Epoch 509 | Train Loss: 0.0010 | Test Loss: 0.0011 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 509, saving checkpoint...\n",
      "Epoch 510 | Train Loss: 0.0010 | Test Loss: 0.0010 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 510, saving checkpoint...\n",
      "Epoch 511 | Train Loss: 0.0010 | Test Loss: 0.0010 | LR: 0.001000 | Time: 0.06 sec\n",
      "Epoch 512 | Train Loss: 0.0010 | Test Loss: 0.0010 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 512, saving checkpoint...\n",
      "Epoch 513 | Train Loss: 0.0010 | Test Loss: 0.0010 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 513, saving checkpoint...\n",
      "Epoch 514 | Train Loss: 0.0010 | Test Loss: 0.0010 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 514, saving checkpoint...\n",
      "Epoch 515 | Train Loss: 0.0010 | Test Loss: 0.0010 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 515, saving checkpoint...\n",
      "Epoch 516 | Train Loss: 0.0010 | Test Loss: 0.0010 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 516, saving checkpoint...\n",
      "Epoch 517 | Train Loss: 0.0010 | Test Loss: 0.0010 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 517, saving checkpoint...\n",
      "Epoch 518 | Train Loss: 0.0010 | Test Loss: 0.0010 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 518, saving checkpoint...\n",
      "Epoch 519 | Train Loss: 0.0010 | Test Loss: 0.0010 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 519, saving checkpoint...\n",
      "Epoch 520 | Train Loss: 0.0010 | Test Loss: 0.0010 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 520, saving checkpoint...\n",
      "Epoch 521 | Train Loss: 0.0009 | Test Loss: 0.0010 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 521, saving checkpoint...\n",
      "Epoch 522 | Train Loss: 0.0009 | Test Loss: 0.0010 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 522, saving checkpoint...\n",
      "Epoch 523 | Train Loss: 0.0010 | Test Loss: 0.0010 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 523, saving checkpoint...\n",
      "Epoch 524 | Train Loss: 0.0009 | Test Loss: 0.0010 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 524, saving checkpoint...\n",
      "Epoch 525 | Train Loss: 0.0009 | Test Loss: 0.0010 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 525, saving checkpoint...\n",
      "Epoch 526 | Train Loss: 0.0009 | Test Loss: 0.0010 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 526, saving checkpoint...\n",
      "Epoch 527 | Train Loss: 0.0009 | Test Loss: 0.0010 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 527, saving checkpoint...\n",
      "Epoch 528 | Train Loss: 0.0009 | Test Loss: 0.0010 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 528, saving checkpoint...\n",
      "Epoch 529 | Train Loss: 0.0009 | Test Loss: 0.0010 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 529, saving checkpoint...\n",
      "Epoch 530 | Train Loss: 0.0009 | Test Loss: 0.0010 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 530, saving checkpoint...\n",
      "Epoch 531 | Train Loss: 0.0009 | Test Loss: 0.0010 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 531, saving checkpoint...\n",
      "Epoch 532 | Train Loss: 0.0009 | Test Loss: 0.0010 | LR: 0.001000 | Time: 0.07 sec\n",
      "Epoch 533 | Train Loss: 0.0009 | Test Loss: 0.0009 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 533, saving checkpoint...\n",
      "Epoch 534 | Train Loss: 0.0009 | Test Loss: 0.0009 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 534, saving checkpoint...\n",
      "Epoch 535 | Train Loss: 0.0009 | Test Loss: 0.0009 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 535, saving checkpoint...\n",
      "Epoch 536 | Train Loss: 0.0009 | Test Loss: 0.0009 | LR: 0.001000 | Time: 0.07 sec\n",
      "Epoch 537 | Train Loss: 0.0009 | Test Loss: 0.0009 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 537, saving checkpoint...\n",
      "Epoch 538 | Train Loss: 0.0009 | Test Loss: 0.0009 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 538, saving checkpoint...\n",
      "Epoch 539 | Train Loss: 0.0009 | Test Loss: 0.0009 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 539, saving checkpoint...\n",
      "Epoch 540 | Train Loss: 0.0008 | Test Loss: 0.0009 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 540, saving checkpoint...\n",
      "Epoch 541 | Train Loss: 0.0008 | Test Loss: 0.0009 | LR: 0.001000 | Time: 0.07 sec\n",
      "Epoch 542 | Train Loss: 0.0008 | Test Loss: 0.0009 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 542, saving checkpoint...\n",
      "Epoch 543 | Train Loss: 0.0008 | Test Loss: 0.0009 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 543, saving checkpoint...\n",
      "Epoch 544 | Train Loss: 0.0008 | Test Loss: 0.0009 | LR: 0.001000 | Time: 0.07 sec\n",
      "Epoch 545 | Train Loss: 0.0008 | Test Loss: 0.0009 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 545, saving checkpoint...\n",
      "Epoch 546 | Train Loss: 0.0008 | Test Loss: 0.0009 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 546, saving checkpoint...\n",
      "Epoch 547 | Train Loss: 0.0008 | Test Loss: 0.0009 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 547, saving checkpoint...\n",
      "Epoch 548 | Train Loss: 0.0008 | Test Loss: 0.0009 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 548, saving checkpoint...\n",
      "Epoch 549 | Train Loss: 0.0008 | Test Loss: 0.0009 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 549, saving checkpoint...\n",
      "Epoch 550 | Train Loss: 0.0008 | Test Loss: 0.0009 | LR: 0.001000 | Time: 0.07 sec\n",
      "Epoch 551 | Train Loss: 0.0008 | Test Loss: 0.0009 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 551, saving checkpoint...\n",
      "Epoch 552 | Train Loss: 0.0008 | Test Loss: 0.0009 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 552, saving checkpoint...\n",
      "Epoch 553 | Train Loss: 0.0008 | Test Loss: 0.0009 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 553, saving checkpoint...\n",
      "Epoch 554 | Train Loss: 0.0008 | Test Loss: 0.0009 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 554, saving checkpoint...\n",
      "Epoch 555 | Train Loss: 0.0008 | Test Loss: 0.0009 | LR: 0.001000 | Time: 0.07 sec\n",
      "Epoch 556 | Train Loss: 0.0008 | Test Loss: 0.0008 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 556, saving checkpoint...\n",
      "Epoch 557 | Train Loss: 0.0008 | Test Loss: 0.0008 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 557, saving checkpoint...\n",
      "Epoch 558 | Train Loss: 0.0007 | Test Loss: 0.0008 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 558, saving checkpoint...\n",
      "Epoch 559 | Train Loss: 0.0007 | Test Loss: 0.0008 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 559, saving checkpoint...\n",
      "Epoch 560 | Train Loss: 0.0007 | Test Loss: 0.0008 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 560, saving checkpoint...\n",
      "Epoch 561 | Train Loss: 0.0007 | Test Loss: 0.0008 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 561, saving checkpoint...\n",
      "Epoch 562 | Train Loss: 0.0007 | Test Loss: 0.0008 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 562, saving checkpoint...\n",
      "Epoch 563 | Train Loss: 0.0007 | Test Loss: 0.0008 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 563, saving checkpoint...\n",
      "Epoch 564 | Train Loss: 0.0007 | Test Loss: 0.0008 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 564, saving checkpoint...\n",
      "Epoch 565 | Train Loss: 0.0007 | Test Loss: 0.0008 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 565, saving checkpoint...\n",
      "Epoch 566 | Train Loss: 0.0007 | Test Loss: 0.0008 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 566, saving checkpoint...\n",
      "Epoch 567 | Train Loss: 0.0007 | Test Loss: 0.0008 | LR: 0.001000 | Time: 0.07 sec\n",
      "Epoch 568 | Train Loss: 0.0007 | Test Loss: 0.0008 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 568, saving checkpoint...\n",
      "Epoch 569 | Train Loss: 0.0007 | Test Loss: 0.0008 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 569, saving checkpoint...\n",
      "Epoch 570 | Train Loss: 0.0007 | Test Loss: 0.0008 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 570, saving checkpoint...\n",
      "Epoch 571 | Train Loss: 0.0007 | Test Loss: 0.0008 | LR: 0.001000 | Time: 0.66 sec\n",
      "New best model found at epoch 571, saving checkpoint...\n",
      "Epoch 572 | Train Loss: 0.0007 | Test Loss: 0.0008 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 572, saving checkpoint...\n",
      "Epoch 573 | Train Loss: 0.0007 | Test Loss: 0.0008 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 573, saving checkpoint...\n",
      "Epoch 574 | Train Loss: 0.0007 | Test Loss: 0.0008 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 574, saving checkpoint...\n",
      "Epoch 575 | Train Loss: 0.0007 | Test Loss: 0.0008 | LR: 0.001000 | Time: 0.07 sec\n",
      "Epoch 576 | Train Loss: 0.0007 | Test Loss: 0.0008 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 576, saving checkpoint...\n",
      "Epoch 577 | Train Loss: 0.0007 | Test Loss: 0.0008 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 577, saving checkpoint...\n",
      "Epoch 578 | Train Loss: 0.0007 | Test Loss: 0.0008 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 578, saving checkpoint...\n",
      "Epoch 579 | Train Loss: 0.0007 | Test Loss: 0.0007 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 579, saving checkpoint...\n",
      "Epoch 580 | Train Loss: 0.0007 | Test Loss: 0.0007 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 580, saving checkpoint...\n",
      "Epoch 581 | Train Loss: 0.0006 | Test Loss: 0.0007 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 581, saving checkpoint...\n",
      "Epoch 582 | Train Loss: 0.0006 | Test Loss: 0.0007 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 582, saving checkpoint...\n",
      "Epoch 583 | Train Loss: 0.0006 | Test Loss: 0.0007 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 583, saving checkpoint...\n",
      "Epoch 584 | Train Loss: 0.0007 | Test Loss: 0.0007 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 584, saving checkpoint...\n",
      "Epoch 585 | Train Loss: 0.0006 | Test Loss: 0.0007 | LR: 0.001000 | Time: 0.07 sec\n",
      "Epoch 586 | Train Loss: 0.0006 | Test Loss: 0.0007 | LR: 0.001000 | Time: 0.08 sec\n",
      "Epoch 587 | Train Loss: 0.0006 | Test Loss: 0.0007 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 587, saving checkpoint...\n",
      "Epoch 588 | Train Loss: 0.0006 | Test Loss: 0.0007 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 588, saving checkpoint...\n",
      "Epoch 589 | Train Loss: 0.0006 | Test Loss: 0.0007 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 589, saving checkpoint...\n",
      "Epoch 590 | Train Loss: 0.0006 | Test Loss: 0.0007 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 590, saving checkpoint...\n",
      "Epoch 591 | Train Loss: 0.0006 | Test Loss: 0.0007 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 591, saving checkpoint...\n",
      "Epoch 592 | Train Loss: 0.0006 | Test Loss: 0.0007 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 592, saving checkpoint...\n",
      "Epoch 593 | Train Loss: 0.0006 | Test Loss: 0.0007 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 593, saving checkpoint...\n",
      "Epoch 594 | Train Loss: 0.0006 | Test Loss: 0.0007 | LR: 0.001000 | Time: 0.06 sec\n",
      "Epoch 595 | Train Loss: 0.0006 | Test Loss: 0.0007 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 595, saving checkpoint...\n",
      "Epoch 596 | Train Loss: 0.0006 | Test Loss: 0.0007 | LR: 0.001000 | Time: 0.06 sec\n",
      "Epoch 597 | Train Loss: 0.0006 | Test Loss: 0.0007 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 597, saving checkpoint...\n",
      "Epoch 598 | Train Loss: 0.0006 | Test Loss: 0.0007 | LR: 0.001000 | Time: 0.06 sec\n",
      "Epoch 599 | Train Loss: 0.0006 | Test Loss: 0.0007 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 599, saving checkpoint...\n",
      "Epoch 600 | Train Loss: 0.0006 | Test Loss: 0.0007 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 600, saving checkpoint...\n",
      "Epoch 601 | Train Loss: 0.0006 | Test Loss: 0.0007 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 601, saving checkpoint...\n",
      "Epoch 602 | Train Loss: 0.0006 | Test Loss: 0.0007 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 602, saving checkpoint...\n",
      "Epoch 603 | Train Loss: 0.0006 | Test Loss: 0.0007 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 603, saving checkpoint...\n",
      "Epoch 604 | Train Loss: 0.0006 | Test Loss: 0.0007 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 604, saving checkpoint...\n",
      "Epoch 605 | Train Loss: 0.0006 | Test Loss: 0.0007 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 605, saving checkpoint...\n",
      "Epoch 606 | Train Loss: 0.0006 | Test Loss: 0.0007 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 606, saving checkpoint...\n",
      "Epoch 607 | Train Loss: 0.0005 | Test Loss: 0.0007 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 607, saving checkpoint...\n",
      "Epoch 608 | Train Loss: 0.0005 | Test Loss: 0.0006 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 608, saving checkpoint...\n",
      "Epoch 609 | Train Loss: 0.0005 | Test Loss: 0.0006 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 609, saving checkpoint...\n",
      "Epoch 610 | Train Loss: 0.0005 | Test Loss: 0.0006 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 610, saving checkpoint...\n",
      "Epoch 611 | Train Loss: 0.0005 | Test Loss: 0.0006 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 611, saving checkpoint...\n",
      "Epoch 612 | Train Loss: 0.0005 | Test Loss: 0.0006 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 612, saving checkpoint...\n",
      "Epoch 613 | Train Loss: 0.0005 | Test Loss: 0.0006 | LR: 0.001000 | Time: 0.06 sec\n",
      "Epoch 614 | Train Loss: 0.0005 | Test Loss: 0.0006 | LR: 0.001000 | Time: 0.06 sec\n",
      "Epoch 615 | Train Loss: 0.0005 | Test Loss: 0.0006 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 615, saving checkpoint...\n",
      "Epoch 616 | Train Loss: 0.0005 | Test Loss: 0.0006 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 616, saving checkpoint...\n",
      "Epoch 617 | Train Loss: 0.0005 | Test Loss: 0.0006 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 617, saving checkpoint...\n",
      "Epoch 618 | Train Loss: 0.0005 | Test Loss: 0.0006 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 618, saving checkpoint...\n",
      "Epoch 619 | Train Loss: 0.0005 | Test Loss: 0.0006 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 619, saving checkpoint...\n",
      "Epoch 620 | Train Loss: 0.0005 | Test Loss: 0.0006 | LR: 0.001000 | Time: 0.06 sec\n",
      "Epoch 621 | Train Loss: 0.0005 | Test Loss: 0.0006 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 621, saving checkpoint...\n",
      "Epoch 622 | Train Loss: 0.0005 | Test Loss: 0.0006 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 622, saving checkpoint...\n",
      "Epoch 623 | Train Loss: 0.0005 | Test Loss: 0.0006 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 623, saving checkpoint...\n",
      "Epoch 624 | Train Loss: 0.0005 | Test Loss: 0.0006 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 624, saving checkpoint...\n",
      "Epoch 625 | Train Loss: 0.0005 | Test Loss: 0.0006 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 625, saving checkpoint...\n",
      "Epoch 626 | Train Loss: 0.0005 | Test Loss: 0.0006 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 626, saving checkpoint...\n",
      "Epoch 627 | Train Loss: 0.0005 | Test Loss: 0.0006 | LR: 0.001000 | Time: 0.06 sec\n",
      "Epoch 628 | Train Loss: 0.0005 | Test Loss: 0.0006 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 628, saving checkpoint...\n",
      "Epoch 629 | Train Loss: 0.0004 | Test Loss: 0.0005 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 629, saving checkpoint...\n",
      "Epoch 630 | Train Loss: 0.0005 | Test Loss: 0.0005 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 630, saving checkpoint...\n",
      "Epoch 631 | Train Loss: 0.0004 | Test Loss: 0.0005 | LR: 0.001000 | Time: 0.08 sec\n",
      "New best model found at epoch 631, saving checkpoint...\n",
      "Epoch 632 | Train Loss: 0.0004 | Test Loss: 0.0005 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 632, saving checkpoint...\n",
      "Epoch 633 | Train Loss: 0.0004 | Test Loss: 0.0005 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 633, saving checkpoint...\n",
      "Epoch 634 | Train Loss: 0.0004 | Test Loss: 0.0005 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 634, saving checkpoint...\n",
      "Epoch 635 | Train Loss: 0.0004 | Test Loss: 0.0005 | LR: 0.001000 | Time: 0.06 sec\n",
      "Epoch 636 | Train Loss: 0.0004 | Test Loss: 0.0005 | LR: 0.001000 | Time: 0.06 sec\n",
      "Epoch 637 | Train Loss: 0.0004 | Test Loss: 0.0005 | LR: 0.001000 | Time: 0.06 sec\n",
      "Epoch 638 | Train Loss: 0.0004 | Test Loss: 0.0005 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 638, saving checkpoint...\n",
      "Epoch 639 | Train Loss: 0.0004 | Test Loss: 0.0005 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 639, saving checkpoint...\n",
      "Epoch 640 | Train Loss: 0.0004 | Test Loss: 0.0005 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 640, saving checkpoint...\n",
      "Epoch 641 | Train Loss: 0.0004 | Test Loss: 0.0005 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 641, saving checkpoint...\n",
      "Epoch 642 | Train Loss: 0.0004 | Test Loss: 0.0005 | LR: 0.001000 | Time: 0.06 sec\n",
      "Epoch 643 | Train Loss: 0.0004 | Test Loss: 0.0005 | LR: 0.001000 | Time: 0.06 sec\n",
      "Epoch 644 | Train Loss: 0.0004 | Test Loss: 0.0005 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 644, saving checkpoint...\n",
      "Epoch 645 | Train Loss: 0.0004 | Test Loss: 0.0005 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 645, saving checkpoint...\n",
      "Epoch 646 | Train Loss: 0.0004 | Test Loss: 0.0005 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 646, saving checkpoint...\n",
      "Epoch 647 | Train Loss: 0.0004 | Test Loss: 0.0005 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 647, saving checkpoint...\n",
      "Epoch 648 | Train Loss: 0.0004 | Test Loss: 0.0005 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 648, saving checkpoint...\n",
      "Epoch 649 | Train Loss: 0.0004 | Test Loss: 0.0005 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 649, saving checkpoint...\n",
      "Epoch 650 | Train Loss: 0.0004 | Test Loss: 0.0005 | LR: 0.001000 | Time: 0.06 sec\n",
      "Epoch 651 | Train Loss: 0.0004 | Test Loss: 0.0005 | LR: 0.001000 | Time: 0.06 sec\n",
      "Epoch 652 | Train Loss: 0.0004 | Test Loss: 0.0005 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 652, saving checkpoint...\n",
      "Epoch 653 | Train Loss: 0.0004 | Test Loss: 0.0005 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 653, saving checkpoint...\n",
      "Epoch 654 | Train Loss: 0.0004 | Test Loss: 0.0005 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 654, saving checkpoint...\n",
      "Epoch 655 | Train Loss: 0.0004 | Test Loss: 0.0005 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 655, saving checkpoint...\n",
      "Epoch 656 | Train Loss: 0.0004 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 656, saving checkpoint...\n",
      "Epoch 657 | Train Loss: 0.0004 | Test Loss: 0.0005 | LR: 0.001000 | Time: 0.06 sec\n",
      "Epoch 658 | Train Loss: 0.0004 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.06 sec\n",
      "Epoch 659 | Train Loss: 0.0004 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 659, saving checkpoint...\n",
      "Epoch 660 | Train Loss: 0.0004 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 660, saving checkpoint...\n",
      "Epoch 661 | Train Loss: 0.0004 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 661, saving checkpoint...\n",
      "Epoch 662 | Train Loss: 0.0004 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 662, saving checkpoint...\n",
      "Epoch 663 | Train Loss: 0.0004 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 663, saving checkpoint...\n",
      "Epoch 664 | Train Loss: 0.0004 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 664, saving checkpoint...\n",
      "Epoch 665 | Train Loss: 0.0003 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 665, saving checkpoint...\n",
      "Epoch 666 | Train Loss: 0.0004 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 666, saving checkpoint...\n",
      "Epoch 667 | Train Loss: 0.0003 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 667, saving checkpoint...\n",
      "Epoch 668 | Train Loss: 0.0003 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 668, saving checkpoint...\n",
      "Epoch 669 | Train Loss: 0.0003 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.06 sec\n",
      "Epoch 670 | Train Loss: 0.0003 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 670, saving checkpoint...\n",
      "Epoch 671 | Train Loss: 0.0003 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.34 sec\n",
      "Epoch 672 | Train Loss: 0.0003 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 672, saving checkpoint...\n",
      "Epoch 673 | Train Loss: 0.0003 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 673, saving checkpoint...\n",
      "Epoch 674 | Train Loss: 0.0003 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 674, saving checkpoint...\n",
      "Epoch 675 | Train Loss: 0.0003 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 675, saving checkpoint...\n",
      "Epoch 676 | Train Loss: 0.0003 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 676, saving checkpoint...\n",
      "Epoch 677 | Train Loss: 0.0003 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.07 sec\n",
      "Epoch 678 | Train Loss: 0.0003 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.07 sec\n",
      "Epoch 679 | Train Loss: 0.0003 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.06 sec\n",
      "Epoch 680 | Train Loss: 0.0003 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.06 sec\n",
      "Epoch 681 | Train Loss: 0.0003 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 681, saving checkpoint...\n",
      "Epoch 682 | Train Loss: 0.0003 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.07 sec\n",
      "Epoch 683 | Train Loss: 0.0003 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 683, saving checkpoint...\n",
      "Epoch 684 | Train Loss: 0.0003 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 684, saving checkpoint...\n",
      "Epoch 685 | Train Loss: 0.0003 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 685, saving checkpoint...\n",
      "Epoch 686 | Train Loss: 0.0003 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.06 sec\n",
      "Epoch 687 | Train Loss: 0.0003 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.07 sec\n",
      "Epoch 688 | Train Loss: 0.0003 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.06 sec\n",
      "Epoch 689 | Train Loss: 0.0003 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 689, saving checkpoint...\n",
      "Epoch 690 | Train Loss: 0.0003 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 690, saving checkpoint...\n",
      "Epoch 691 | Train Loss: 0.0003 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 691, saving checkpoint...\n",
      "Epoch 692 | Train Loss: 0.0003 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.06 sec\n",
      "Epoch 693 | Train Loss: 0.0003 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 693, saving checkpoint...\n",
      "Epoch 694 | Train Loss: 0.0003 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.06 sec\n",
      "Epoch 695 | Train Loss: 0.0003 | Test Loss: 0.0004 | LR: 0.001000 | Time: 0.07 sec\n",
      "Epoch 696 | Train Loss: 0.0003 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 696, saving checkpoint...\n",
      "Epoch 697 | Train Loss: 0.0003 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 697, saving checkpoint...\n",
      "Epoch 698 | Train Loss: 0.0003 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "Epoch 699 | Train Loss: 0.0003 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 699, saving checkpoint...\n",
      "Epoch 700 | Train Loss: 0.0003 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 700, saving checkpoint...\n",
      "Epoch 701 | Train Loss: 0.0003 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 701, saving checkpoint...\n",
      "Epoch 702 | Train Loss: 0.0003 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 702, saving checkpoint...\n",
      "Epoch 703 | Train Loss: 0.0003 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.06 sec\n",
      "Epoch 704 | Train Loss: 0.0003 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.06 sec\n",
      "Epoch 705 | Train Loss: 0.0003 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 705, saving checkpoint...\n",
      "Epoch 706 | Train Loss: 0.0003 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 706, saving checkpoint...\n",
      "Epoch 707 | Train Loss: 0.0003 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 707, saving checkpoint...\n",
      "Epoch 708 | Train Loss: 0.0003 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.06 sec\n",
      "Epoch 709 | Train Loss: 0.0003 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "Epoch 710 | Train Loss: 0.0003 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.06 sec\n",
      "Epoch 711 | Train Loss: 0.0003 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "Epoch 712 | Train Loss: 0.0003 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 712, saving checkpoint...\n",
      "Epoch 713 | Train Loss: 0.0003 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.06 sec\n",
      "Epoch 714 | Train Loss: 0.0003 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 714, saving checkpoint...\n",
      "Epoch 715 | Train Loss: 0.0003 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 715, saving checkpoint...\n",
      "Epoch 716 | Train Loss: 0.0003 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 716, saving checkpoint...\n",
      "Epoch 717 | Train Loss: 0.0003 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 717, saving checkpoint...\n",
      "Epoch 718 | Train Loss: 0.0003 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 718, saving checkpoint...\n",
      "Epoch 719 | Train Loss: 0.0003 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 719, saving checkpoint...\n",
      "Epoch 720 | Train Loss: 0.0003 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.06 sec\n",
      "Epoch 721 | Train Loss: 0.0003 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 721, saving checkpoint...\n",
      "Epoch 722 | Train Loss: 0.0003 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "Epoch 723 | Train Loss: 0.0003 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 723, saving checkpoint...\n",
      "Epoch 724 | Train Loss: 0.0003 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 724, saving checkpoint...\n",
      "Epoch 725 | Train Loss: 0.0003 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "Epoch 726 | Train Loss: 0.0003 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.06 sec\n",
      "Epoch 727 | Train Loss: 0.0003 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "Epoch 728 | Train Loss: 0.0003 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "Epoch 729 | Train Loss: 0.0003 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 729, saving checkpoint...\n",
      "Epoch 730 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 730, saving checkpoint...\n",
      "Epoch 731 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "Epoch 732 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 732, saving checkpoint...\n",
      "Epoch 733 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 733, saving checkpoint...\n",
      "Epoch 734 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 734, saving checkpoint...\n",
      "Epoch 735 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 735, saving checkpoint...\n",
      "Epoch 736 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "Epoch 737 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.06 sec\n",
      "New best model found at epoch 737, saving checkpoint...\n",
      "Epoch 738 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "Epoch 739 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "Epoch 740 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 740, saving checkpoint...\n",
      "Epoch 741 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 741, saving checkpoint...\n",
      "Epoch 742 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "Epoch 743 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 743, saving checkpoint...\n",
      "Epoch 744 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 744, saving checkpoint...\n",
      "Epoch 745 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 745, saving checkpoint...\n",
      "Epoch 746 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 746, saving checkpoint...\n",
      "Epoch 747 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 747, saving checkpoint...\n",
      "Epoch 748 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 748, saving checkpoint...\n",
      "Epoch 749 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 749, saving checkpoint...\n",
      "Epoch 750 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.06 sec\n",
      "Epoch 751 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "Epoch 752 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "New best model found at epoch 752, saving checkpoint...\n",
      "Epoch 753 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.06 sec\n",
      "Epoch 754 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "Epoch 755 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "Epoch 756 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "Epoch 757 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "Epoch 758 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.001000 | Time: 0.07 sec\n",
      "Epoch 759 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.000500 | Time: 0.07 sec\n",
      "Epoch 760 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.000500 | Time: 0.07 sec\n",
      "Epoch 761 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 761, saving checkpoint...\n",
      "Epoch 762 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 762, saving checkpoint...\n",
      "Epoch 763 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 763, saving checkpoint...\n",
      "Epoch 764 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 764, saving checkpoint...\n",
      "Epoch 765 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.000500 | Time: 0.07 sec\n",
      "Epoch 766 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 766, saving checkpoint...\n",
      "Epoch 767 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 767, saving checkpoint...\n",
      "Epoch 768 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 768, saving checkpoint...\n",
      "Epoch 769 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 769, saving checkpoint...\n",
      "Epoch 770 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 770, saving checkpoint...\n",
      "Epoch 771 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.000500 | Time: 0.07 sec\n",
      "Epoch 772 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 772, saving checkpoint...\n",
      "Epoch 773 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.000500 | Time: 0.07 sec\n",
      "Epoch 774 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 774, saving checkpoint...\n",
      "Epoch 775 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 775, saving checkpoint...\n",
      "Epoch 776 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 776, saving checkpoint...\n",
      "Epoch 777 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 777, saving checkpoint...\n",
      "Epoch 778 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 778, saving checkpoint...\n",
      "Epoch 779 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.000500 | Time: 0.07 sec\n",
      "Epoch 780 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 780, saving checkpoint...\n",
      "Epoch 781 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 781, saving checkpoint...\n",
      "Epoch 782 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 782, saving checkpoint...\n",
      "Epoch 783 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 783, saving checkpoint...\n",
      "Epoch 784 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 784, saving checkpoint...\n",
      "Epoch 785 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 785, saving checkpoint...\n",
      "Epoch 786 | Train Loss: 0.0002 | Test Loss: 0.0003 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 786, saving checkpoint...\n",
      "Epoch 787 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 787, saving checkpoint...\n",
      "Epoch 788 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 788, saving checkpoint...\n",
      "Epoch 789 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.07 sec\n",
      "Epoch 790 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.07 sec\n",
      "Epoch 791 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 791, saving checkpoint...\n",
      "Epoch 792 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 792, saving checkpoint...\n",
      "Epoch 793 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 793, saving checkpoint...\n",
      "Epoch 794 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 794, saving checkpoint...\n",
      "Epoch 795 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.07 sec\n",
      "Epoch 796 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 796, saving checkpoint...\n",
      "Epoch 797 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 797, saving checkpoint...\n",
      "Epoch 798 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.07 sec\n",
      "Epoch 799 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 799, saving checkpoint...\n",
      "Epoch 800 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.07 sec\n",
      "Epoch 801 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.07 sec\n",
      "Epoch 802 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.07 sec\n",
      "Epoch 803 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.08 sec\n",
      "New best model found at epoch 803, saving checkpoint...\n",
      "Epoch 804 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 804, saving checkpoint...\n",
      "Epoch 805 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 805, saving checkpoint...\n",
      "Epoch 806 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.07 sec\n",
      "Epoch 807 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 807, saving checkpoint...\n",
      "Epoch 808 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 808, saving checkpoint...\n",
      "Epoch 809 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.08 sec\n",
      "New best model found at epoch 809, saving checkpoint...\n",
      "Epoch 810 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.08 sec\n",
      "New best model found at epoch 810, saving checkpoint...\n",
      "Epoch 811 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.08 sec\n",
      "New best model found at epoch 811, saving checkpoint...\n",
      "Epoch 812 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.08 sec\n",
      "New best model found at epoch 812, saving checkpoint...\n",
      "Epoch 813 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.08 sec\n",
      "New best model found at epoch 813, saving checkpoint...\n",
      "Epoch 814 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.08 sec\n",
      "Epoch 815 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.08 sec\n",
      "New best model found at epoch 815, saving checkpoint...\n",
      "Epoch 816 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.08 sec\n",
      "New best model found at epoch 816, saving checkpoint...\n",
      "Epoch 817 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.09 sec\n",
      "New best model found at epoch 817, saving checkpoint...\n",
      "Epoch 818 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.08 sec\n",
      "New best model found at epoch 818, saving checkpoint...\n",
      "Epoch 819 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.08 sec\n",
      "New best model found at epoch 819, saving checkpoint...\n",
      "Epoch 820 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.08 sec\n",
      "New best model found at epoch 820, saving checkpoint...\n",
      "Epoch 821 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 821, saving checkpoint...\n",
      "Epoch 822 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.09 sec\n",
      "New best model found at epoch 822, saving checkpoint...\n",
      "Epoch 823 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.08 sec\n",
      "New best model found at epoch 823, saving checkpoint...\n",
      "Epoch 824 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.08 sec\n",
      "New best model found at epoch 824, saving checkpoint...\n",
      "Epoch 825 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.07 sec\n",
      "Epoch 826 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 826, saving checkpoint...\n",
      "Epoch 827 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 827, saving checkpoint...\n",
      "Epoch 828 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.07 sec\n",
      "Epoch 829 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 829, saving checkpoint...\n",
      "Epoch 830 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 830, saving checkpoint...\n",
      "Epoch 831 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 831, saving checkpoint...\n",
      "Epoch 832 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 832, saving checkpoint...\n",
      "Epoch 833 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.07 sec\n",
      "New best model found at epoch 833, saving checkpoint...\n",
      "Epoch 834 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.08 sec\n",
      "New best model found at epoch 834, saving checkpoint...\n",
      "Epoch 835 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.55 sec\n",
      "Epoch 836 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 836, saving checkpoint...\n",
      "Epoch 837 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "Epoch 838 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 838, saving checkpoint...\n",
      "Epoch 839 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 839, saving checkpoint...\n",
      "Epoch 840 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 840, saving checkpoint...\n",
      "Epoch 841 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 841, saving checkpoint...\n",
      "Epoch 842 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 842, saving checkpoint...\n",
      "Epoch 843 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "Epoch 844 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 844, saving checkpoint...\n",
      "Epoch 845 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.12 sec\n",
      "Epoch 846 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 846, saving checkpoint...\n",
      "Epoch 847 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "Epoch 848 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 848, saving checkpoint...\n",
      "Epoch 849 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 849, saving checkpoint...\n",
      "Epoch 850 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 850, saving checkpoint...\n",
      "Epoch 851 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 851, saving checkpoint...\n",
      "Epoch 852 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 852, saving checkpoint...\n",
      "Epoch 853 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.12 sec\n",
      "New best model found at epoch 853, saving checkpoint...\n",
      "Epoch 854 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 854, saving checkpoint...\n",
      "Epoch 855 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 855, saving checkpoint...\n",
      "Epoch 856 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 856, saving checkpoint...\n",
      "Epoch 857 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "Epoch 858 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 858, saving checkpoint...\n",
      "Epoch 859 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 859, saving checkpoint...\n",
      "Epoch 860 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 860, saving checkpoint...\n",
      "Epoch 861 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 861, saving checkpoint...\n",
      "Epoch 862 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "Epoch 863 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 863, saving checkpoint...\n",
      "Epoch 864 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 864, saving checkpoint...\n",
      "Epoch 865 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 865, saving checkpoint...\n",
      "Epoch 866 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 866, saving checkpoint...\n",
      "Epoch 867 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 867, saving checkpoint...\n",
      "Epoch 868 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 868, saving checkpoint...\n",
      "Epoch 869 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 869, saving checkpoint...\n",
      "Epoch 870 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 870, saving checkpoint...\n",
      "Epoch 871 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 871, saving checkpoint...\n",
      "Epoch 872 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 872, saving checkpoint...\n",
      "Epoch 873 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 873, saving checkpoint...\n",
      "Epoch 874 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.12 sec\n",
      "New best model found at epoch 874, saving checkpoint...\n",
      "Epoch 875 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 875, saving checkpoint...\n",
      "Epoch 876 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "Epoch 877 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.12 sec\n",
      "New best model found at epoch 877, saving checkpoint...\n",
      "Epoch 878 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "Epoch 879 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.12 sec\n",
      "New best model found at epoch 879, saving checkpoint...\n",
      "Epoch 880 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 880, saving checkpoint...\n",
      "Epoch 881 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 881, saving checkpoint...\n",
      "Epoch 882 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "Epoch 883 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 883, saving checkpoint...\n",
      "Epoch 884 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "Epoch 885 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 885, saving checkpoint...\n",
      "Epoch 886 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "Epoch 887 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 887, saving checkpoint...\n",
      "Epoch 888 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 888, saving checkpoint...\n",
      "Epoch 889 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 889, saving checkpoint...\n",
      "Epoch 890 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 890, saving checkpoint...\n",
      "Epoch 891 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 891, saving checkpoint...\n",
      "Epoch 892 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 892, saving checkpoint...\n",
      "Epoch 893 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 893, saving checkpoint...\n",
      "Epoch 894 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "Epoch 895 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "Epoch 896 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 896, saving checkpoint...\n",
      "Epoch 897 | Train Loss: 0.0002 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "Epoch 898 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "Epoch 899 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 899, saving checkpoint...\n",
      "Epoch 900 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 900, saving checkpoint...\n",
      "Epoch 901 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 901, saving checkpoint...\n",
      "Epoch 902 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 902, saving checkpoint...\n",
      "Epoch 903 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 903, saving checkpoint...\n",
      "Epoch 904 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 904, saving checkpoint...\n",
      "Epoch 905 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 905, saving checkpoint...\n",
      "Epoch 906 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 906, saving checkpoint...\n",
      "Epoch 907 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 907, saving checkpoint...\n",
      "Epoch 908 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "Epoch 909 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 909, saving checkpoint...\n",
      "Epoch 910 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 910, saving checkpoint...\n",
      "Epoch 911 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 911, saving checkpoint...\n",
      "Epoch 912 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 912, saving checkpoint...\n",
      "Epoch 913 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 913, saving checkpoint...\n",
      "Epoch 914 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 914, saving checkpoint...\n",
      "Epoch 915 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 915, saving checkpoint...\n",
      "Epoch 916 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 916, saving checkpoint...\n",
      "Epoch 917 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 917, saving checkpoint...\n",
      "Epoch 918 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 918, saving checkpoint...\n",
      "Epoch 919 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 919, saving checkpoint...\n",
      "Epoch 920 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 920, saving checkpoint...\n",
      "Epoch 921 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "Epoch 922 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.12 sec\n",
      "Epoch 923 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.12 sec\n",
      "New best model found at epoch 923, saving checkpoint...\n",
      "Epoch 924 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 924, saving checkpoint...\n",
      "Epoch 925 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "Epoch 926 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "Epoch 927 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 927, saving checkpoint...\n",
      "Epoch 928 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 928, saving checkpoint...\n",
      "Epoch 929 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 929, saving checkpoint...\n",
      "Epoch 930 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.12 sec\n",
      "New best model found at epoch 930, saving checkpoint...\n",
      "Epoch 931 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "Epoch 932 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 932, saving checkpoint...\n",
      "Epoch 933 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.12 sec\n",
      "New best model found at epoch 933, saving checkpoint...\n",
      "Epoch 934 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 934, saving checkpoint...\n",
      "Epoch 935 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 935, saving checkpoint...\n",
      "Epoch 936 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 936, saving checkpoint...\n",
      "Epoch 937 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "Epoch 938 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "Epoch 939 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "Epoch 940 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 940, saving checkpoint...\n",
      "Epoch 941 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "Epoch 942 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 942, saving checkpoint...\n",
      "Epoch 943 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "Epoch 944 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 944, saving checkpoint...\n",
      "Epoch 945 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 945, saving checkpoint...\n",
      "Epoch 946 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 946, saving checkpoint...\n",
      "Epoch 947 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 947, saving checkpoint...\n",
      "Epoch 948 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.70 sec\n",
      "New best model found at epoch 948, saving checkpoint...\n",
      "Epoch 949 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 949, saving checkpoint...\n",
      "Epoch 950 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "Epoch 951 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 951, saving checkpoint...\n",
      "Epoch 952 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "Epoch 953 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 953, saving checkpoint...\n",
      "Epoch 954 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 954, saving checkpoint...\n",
      "Epoch 955 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.13 sec\n",
      "New best model found at epoch 955, saving checkpoint...\n",
      "Epoch 956 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 956, saving checkpoint...\n",
      "Epoch 957 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "Epoch 958 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "Epoch 959 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "Epoch 960 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "Epoch 961 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 961, saving checkpoint...\n",
      "Epoch 962 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 962, saving checkpoint...\n",
      "Epoch 963 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 963, saving checkpoint...\n",
      "Epoch 964 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.13 sec\n",
      "Epoch 965 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.13 sec\n",
      "New best model found at epoch 965, saving checkpoint...\n",
      "Epoch 966 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.12 sec\n",
      "Epoch 967 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 967, saving checkpoint...\n",
      "Epoch 968 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "Epoch 969 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 969, saving checkpoint...\n",
      "Epoch 970 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "Epoch 971 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 971, saving checkpoint...\n",
      "Epoch 972 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 972, saving checkpoint...\n",
      "Epoch 973 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 973, saving checkpoint...\n",
      "Epoch 974 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.10 sec\n",
      "New best model found at epoch 974, saving checkpoint...\n",
      "Epoch 975 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "Epoch 976 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "Epoch 977 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.12 sec\n",
      "Epoch 978 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 978, saving checkpoint...\n",
      "Epoch 979 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "Epoch 980 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "Epoch 981 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.12 sec\n",
      "New best model found at epoch 981, saving checkpoint...\n",
      "Epoch 982 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 982, saving checkpoint...\n",
      "Epoch 983 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.12 sec\n",
      "New best model found at epoch 983, saving checkpoint...\n",
      "Epoch 984 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.12 sec\n",
      "New best model found at epoch 984, saving checkpoint...\n",
      "Epoch 985 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.12 sec\n",
      "New best model found at epoch 985, saving checkpoint...\n",
      "Epoch 986 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.12 sec\n",
      "Epoch 987 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.12 sec\n",
      "New best model found at epoch 987, saving checkpoint...\n",
      "Epoch 988 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 988, saving checkpoint...\n",
      "Epoch 989 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.12 sec\n",
      "New best model found at epoch 989, saving checkpoint...\n",
      "Epoch 990 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.12 sec\n",
      "Epoch 991 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.13 sec\n",
      "Epoch 992 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.12 sec\n",
      "New best model found at epoch 992, saving checkpoint...\n",
      "Epoch 993 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.12 sec\n",
      "New best model found at epoch 993, saving checkpoint...\n",
      "Epoch 994 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.12 sec\n",
      "New best model found at epoch 994, saving checkpoint...\n",
      "Epoch 995 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.12 sec\n",
      "New best model found at epoch 995, saving checkpoint...\n",
      "Epoch 996 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "Epoch 997 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.12 sec\n",
      "New best model found at epoch 997, saving checkpoint...\n",
      "Epoch 998 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "New best model found at epoch 998, saving checkpoint...\n",
      "Epoch 999 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n",
      "Epoch 1000 | Train Loss: 0.0001 | Test Loss: 0.0002 | LR: 0.000500 | Time: 0.11 sec\n"
     ]
    }
   ],
   "source": [
    "#Training loop\n",
    "# Prepare training and test datasets\n",
    "train_ds = TitanicAutoencoderDataset(X_train)\n",
    "test_ds = TitanicAutoencoderDataset(X_test)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, num_workers=WORKERS, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, num_workers=WORKERS)\n",
    "\n",
    "# Track losses\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "best_loss = float('inf') # Initialize best_loss for comparison and checkpointing\n",
    "# Training loop\n",
    "num_epochs = 1500\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        recon = model(x)\n",
    "        loss = loss_fn(recon, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_duration = end_time - start_time\n",
    "    # GPU metrics\n",
    "    gpu_alloc = torch.cuda.memory_allocated() / 1024**2 # Convert to MB\n",
    "    gpu_reserved = torch.cuda.memory_reserved() / 1024**2 # Convert to MB\n",
    "    writer.add_scalar(\"GPU/Memory_Allocated_MB\", gpu_alloc, epoch)\n",
    "    writer.add_scalar(\"GPU/Memory_Reserved_MB\", gpu_reserved, epoch)\n",
    "\n",
    "    try:\n",
    "        gpu_util, mem_used = get_gpu_utilization()\n",
    "        writer.add_scalar(\"GPU/Utilization_%\", gpu_util, epoch)\n",
    "        writer.add_scalar(\"GPU/Memory_Used_MB\", mem_used, epoch)\n",
    "    except:\n",
    "        pass  # In case nvidia-smi isn't available\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            recon = model(x)\n",
    "            loss = loss_fn(recon, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    test_losses.append(avg_test_loss)\n",
    "\n",
    "    # ‚úÖ Log scalar losses\n",
    "    writer.add_scalar(\"Loss/train\", avg_train_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/test\", avg_test_loss, epoch)\n",
    "    writer.add_figure(\"Loss Overlap Curve\", plot_loss_curve(train_losses, test_losses), global_step=epoch)\n",
    "\n",
    "    # ‚úÖ Log weights and gradients\n",
    "    for name, param in model.named_parameters():\n",
    "        writer.add_histogram(f\"Weights/{name}\", param, epoch)\n",
    "        if param.grad is not None:\n",
    "            writer.add_histogram(f\"Gradients/{name}\", param.grad, epoch)\n",
    "\n",
    "    # ‚úÖ Log activation from encoder\n",
    "    with torch.no_grad():\n",
    "        activation_sample = torch.tensor(X_train[:1], dtype=torch.float32).to(device)\n",
    "        encoded = model.encoder(activation_sample)\n",
    "        writer.add_histogram(\"Activations/EncoderOutput\", encoded, epoch)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        writer.add_scalar(\"LR\", current_lr, epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:2d} | Train Loss: {avg_train_loss:.4f} | Test Loss: {avg_test_loss:.4f} | LR: {current_lr:.6f} | Time: {epoch_duration:.2f} sec\")\n",
    "\n",
    "    # Save only the best model based on test loss\n",
    "    if avg_test_loss < best_loss:\n",
    "        best_loss = avg_test_loss\n",
    "        print(f\"New best model found at epoch {epoch+1}, saving checkpoint...\")\n",
    "        \n",
    "            # Save the model state\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_losses': train_losses,\n",
    "            'test_losses': test_losses,\n",
    "            'input_dim': X_train.shape[1],           # ‚úÖ Save architecture args\n",
    "            'latent_dim': LATENT_DIM \n",
    "            }, f\"{CHECKPOINT_DIR}/autoencoder_epoch{epoch+1}.pt\")\n",
    "    \n",
    "    scheduler.step(avg_test_loss) # Adjust learning rate based on test loss\n",
    "\n",
    "# ‚úÖ After training ‚Äî log embeddings to projector\n",
    "with torch.no_grad():\n",
    "    sample_input = torch.tensor(X_test[:500], dtype=torch.float32).to(device)\n",
    "    latent_vectors = model.encoder(sample_input)\n",
    "    metadata = [f\"Passenger {i}\" for i in range(sample_input.shape[0])]\n",
    "    writer.add_embedding(latent_vectors, metadata=metadata, tag=\"LatentEmbeddings\", global_step=num_epochs)\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c1ca37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recmd_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
